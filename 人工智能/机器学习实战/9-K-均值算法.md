# 🥧 K-均值算法 K-Means

---

从本节开始进入无监督机器学习方法。在监督学习中，我们的问题是：对于输入变量预测出 Y。而在无监督学习中，目标变量并不事先存在，我们的问题是：**从数据 X 中能发现什么**。

## 1. 聚类

**聚类，简单来说，就是将一个庞杂数据集中具有相似特征的数据自动归类到一起，称为一个簇，簇内的对象越相似，聚类的效果越好**。它是一种无监督的学习(Unsupervised Learning)方法, 不需要预先标注好的训练集。

聚类与分类最大的区别就是分类的目标是否事先已知，例如猫狗识别，你在分类之前已经预先知道要将它分为猫、狗两个种类；而在你聚类之前，你对你的目标是未知的，同样以动物为例，对于一个动物集来说，你并不清楚这个数据集内部有多少种类的动物，你能做的只是利用聚类方法将它自动按照特征分为多类，然后人为给出这个聚类结果的定义（即簇识别）。例如，你将一个动物集分为了三簇（类），然后通过观察这三类动物的特征，你为每一个簇起一个名字，如大象、狗、猫等，这就是聚类的基本思想。

**至于“相似”这一概念，是利用距离这个评价标准来衡量的**，我们通过计算对象与对象之间的距离远近来判断它们是否属于同一类别，即是否是同一个簇。至于距离如何计算，科学家们提出了许多种距离的计算方法，其中**欧式距离**是最为简单和常用的，除此之外还有曼哈顿距离和余弦相似性距离等。

对于x点坐标为(x1,x2,x3,...,xn)和 y点坐标为(y1,y2,y3,...,yn)，两者的欧式距离为:

$d(x,y) ={\sqrt{ (x_{1}-y_{1})^{2}+(x_{2}-y_{2})^{2} + \cdots +(x_{n}-y_{n})^{2} }} ={\sqrt{ \sum_{ {i=1} }^{n}(x_{i}-y_{i})^{2} }}$

在二维平面，它就是我们初中时就学过的两点距离公式

## 2. K-均值聚类算法

### ① 概述

**K-均值（K-Means）** 是发现给定数据集的 K 个簇的聚类算法, 之所以称之为 K-均值 是因为它可以发现 K 个不同的簇, 且**每个簇的中心采用簇中所含值的均值计算而成**。

**簇个数 K 是用户指定的, 每一个簇通过其质心（centroid）, 即簇中所有点的中心来描述**。

**优点**:

- 属于无监督学习，无须准备训练集
- 原理简单，实现起来较为容易
- 结果可解释性较好

**缺点**:

- **需手动设置k值**。 在算法开始预测之前，我们需要手动设置k值，即估计数据大概的类别个数，不合理的k值会使结果缺乏解释性
- 可能收敛到局部最小值, 在大规模数据集上收敛较慢
- 对于异常点、离群点敏感

**适用数据类型** : 数值型数据

### ② K-Means 算法流程

K-Means 算法流程 如下：👇 

- 首先, **随机确定 K 个初始点作为质心（不必是数据中的点）**。
- 然后将数据集中的每个点分配到一个簇中, 具体来讲, 就是为每个点找到距其最近的质心, 并将其分配该质心所对应的簇. 这一步完成之后, 每个簇的质心更新为该簇所有点的平均值. 
- 重复上述过程直到数据集中的所有点都距离它所对应的质心最近时结束。

📑 上述过程的伪代码如下:

```python
- 创建 k 个点作为起始质心（通常是随机选择）
- 当任意一个点的簇分配结果发生改变时（不改变时算法结束）
   - 对数据集中的每个数据点
       - 对每个质心
          - 计算质心与数据点之间的距离
       - 将数据点分配到距其最近的簇
   - 对每一个簇, 计算簇中所有点的均值并将均值作为质心
```

### ③ K-Means 开发流程

- 收集数据: 使用任意方法
- 准备数据: 需要数值型数据类计算距离, 也可以将标称型数据映射为二值型数据再用于距离计算
- 分析数据: 使用任意方法
- 训练算法: 不适用于无监督学习，即无监督学习不需要训练步骤
- 测试算法: 应用聚类算法、观察结果.可以使用量化的误差指标如误差平方和（后面会介绍）来评价算法的结果.
- 使用算法: 可以用于所希望的任何应用.通常情况下, 簇质心可以代表整个簇的数据来做出决策.

### ④ 算法实现

**从文本中构建矩阵**：

```python
import numpy as np

# 从文本中构建矩阵，加载文本文件，然后处理
def loadDataSet(fileName):    # 通用函数，用来解析以 tab 键分隔的 floats（浮点数）
    dataMat = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = list(map(float,curLine))    # 映射所有的元素为 float（浮点数）类型
        dataMat.append(fltLine)
    return dataMat
```

**计算两个向量的欧氏距离**：

```python
# 计算两个向量的欧氏距离
def distEclud(vecA, vecB):
    return np.sqrt(np.sum(np.power(vecA - vecB, 2))) # la.norm(vecA-vecB)
```

**构建一个包含 K 个随机质心的集合**：

```python
# 为给定数据集构建一个包含 k 个随机质心的集合。
# 随机质心必须要在整个数据集的边界之内，可以通过找到数据集每一维的最小和最大值来完成
# 然后生成 0~1.0 之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内。
def randCent(dataSet, k):
    n = np.shape(dataSet)[1] # 列的数量，即数据的特征个数
    centroids = np.mat(np.zeros((k,n))) # 创建k个质心矩阵
    for j in range(n): # 创建随机簇质心，并且在每一维的边界内
        minJ = min(dataSet[:,j])    # 每一维的最小值
        rangeJ = float(max(dataSet[:,j]) - minJ)    # 范围 = 最大值 - 最小值
        centroids[:,j] = np.mat(minJ + rangeJ * np.random.rand(k,1))    # 随机生成(确保随机点在数据的边界之内)
    return centroids
```

OK，以上我们构建了 K-Means 算法所需要的辅助函数，先来测试一下。我们的数据集如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20200724205409.png" style="zoom:67%;" />

```python
dataMat = np.mat(loadDataSet('testSet.txt'))
```

<img src="https://gitee.com/veal98/images/raw/master/img/20200724210037.png" style="zoom:80%;" />

接下来看看我们生成的随机质心矩阵是否在边界范围内：

<img src="https://gitee.com/veal98/images/raw/master/img/20200724210115.png" style="zoom:80%;" />

```python
randCent(dataMat,2)  # 生成两个质心矩阵
```

<img src="https://gitee.com/veal98/images/raw/master/img/20200724210138.png" style="zoom:80%;" />

OK，测试完毕。接下来实现 K-Means 算法。该算法会创建 K 个质心，然后将每个点根据欧式距离分配到最近的质心，再重新计算质心，重复数次，直到分簇结果不再改变为止。

```python
# k-means 聚类算法
# 运行结果（多次运行结果可能会不一样，可以试试，原因为随机质心的影响，但总的结果是对的， 因为数据足够相似，也可能会陷入局部最小值）
def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):
    m = np.shape(dataSet)[0]    # 行数，即数据个数
    # 创建一个与 dataSet 行数一样，但是有两列的矩阵，用来保存簇分配结果.
    # 第 0 列保存簇（质心）的下标，第 1 列保存当前点到簇质心的距离
    clusterAssment = np.mat(np.zeros((m, 2)))    
    centroids = createCent(dataSet, k)    # 创建质心，随机k个质心
    clusterChanged = True # 是否分簇。如果分簇结果不再发生改变，就停止循环
    while clusterChanged:
        clusterChanged = False
        for i in range(m):    # 循环每一个数据点并分配到最近的质心中去
            minDist = np.inf # 最短距离
            minIndex = -1 # 距离哪个下标的质心的距离最小
            for j in range(k):
                distJI = distMeas(centroids[j,:],dataSet[i,:])    # 计算数据点到质心的距离
                if distJI < minDist:    # 如果距离比 minDist（最小距离）还小，更新 minDist（最小距离）和最小质心的 index（索引）
                    minDist = distJI
                    minIndex = j
            if clusterAssment[i, 0] != minIndex:    # 若第 i 个点的簇分配结果改变
                clusterChanged = True    # 簇改变
                clusterAssment[i, :] = minIndex, minDist**2    # 更新簇分配结果为最小质心的 index（索引），minDist（最小距离）的平方
        print("质心：\n",centroids)
        for cent in range(k): # 更新质心
            ptsInClust = dataSet[np.nonzero(clusterAssment[:, 0].A==cent)[0]] # 获取该簇中的所有点
            centroids[cent,:] = np.mean(ptsInClust, axis=0) # 将质心修改为簇中所有点的平均值，mean 就是求平均值的
    return centroids, clusterAssment
```

🏃‍ 运行该代码：

```python
centroids,clusterAssment = kMeans(dataMat, 4) # 分 4 簇
```

![](https://gitee.com/veal98/images/raw/master/img/20200724212029.png)

下图给出了聚类结果的示意图：

<img src="https://gitee.com/veal98/images/raw/master/img/20200724212130.png" style="zoom:80%;" />

## 3. 使用后处理来提高聚类性能

### ① K-Means 聚类算法的缺陷

😒 在 K-Means 的函数测试中，**可能偶尔会陷入局部最小值（局部最优的结果，但不是全局最优的结果）**。

出现这个问题有很多原因，可能是k值取的不合适，可能是距离函数不合适，可能是最初随机选取的质心靠的太近，也可能是数据本身分布的问题。

💬 举个例子：

<img src="https://gitee.com/veal98/images/raw/master/img/20200724212515.png" style="zoom:80%;" />

### ② 后处理

为了解决这个问题，我们可以对生成的簇进行后处理，一种方法是将具有最大**SSE（Sum of Squared Error 误差平方和）**值的簇划分成两个簇。具体实现时可以将最大簇包含的点过滤出来并在这些点上运行 K-均值算法，令 k 设为2。

> 💡 上述代码 `clusterAssment` 中的第 1 列存储的是当前数据点到簇质心的距离，也称为误差

为了保持簇总数不变，可以将某两个簇进行合并。从上图 10-2 中很明显就可以看出，应该将上图下部两个出错的簇质心进行合并。那么问题来了，我们可以很容易对二维数据上的聚类进行可视化， 但是如果遇到40维的数据应该如何去做？

有两种可以量化的办法: 合并最近的质心，或者合并两个使得**SSE**增幅最小的质心。 第一种思路通过计算所有质心之间的距离， 然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总**SSE**值。必须在所有可能的两个簇上重复上述处理过程，直到找到合并最佳的两个簇为止。

因为上述后处理过程实在是有些繁琐，所以有更厉害的大佬提出了另一个称之为**二分K - 均值（bisecting K-Means）**的算法.

## 4. 二分 K-均值算法

### ① 算法流程

⭐ 该算法**<u>首先将所有点作为一个簇，然后将该簇一分为二</u>。之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分时候可以最大程度降低 SSE（平方和误差）的值。**

上述基于 SSE 的划分过程不断重复，直到得到用户指定的簇数目为止。

📑 **二分 K-Means 聚类算法伪代码**：

- 🔺 <u>将所有点看成一个簇</u>
- 当簇数目小于 k 时
- 对于每一个簇
  - 计算总误差
  - 在给定的簇上面进行 KMeans 聚类（k=2）
  - 计算将该簇一分为二之后的<u>总误差</u>
- 选择使得总误差最小的那个簇进行划分操作

### ② 算法实现

✍ **Python 实现**：

```python
# 二分 KMeans 聚类算法, 基于 kMeans 基础之上的优化，以避免陷入局部最小值
def biKMeans(dataSet, k, distMeas=distEclud):
    m = np.shape(dataSet)[0]
    clusterAssment = np.mat(np.zeros((m,2))) # 保存每个数据点的簇分配结果和平方误差
    centroid0 = np.mean(dataSet, axis=0).tolist()[0] # 将所有点看成一个簇，质心初始化为所有数据点的均值
    centList =[centroid0] # 初始化只有 1 个质心的 list
    for j in range(m): # 计算所有数据点到初始质心的距离平方误差
        clusterAssment[j,1] = distMeas(np.mat(centroid0), dataSet[j,:])**2
    while (len(centList) < k): # 当质心数量小于 k 时
        lowestSSE = np.inf
        # 尝试划分每一簇
        for i in range(len(centList)): # 对每一个质心
            ptsInCurrCluster = dataSet[np.nonzero(clusterAssment[:,0].A==i)[0],:] # 获取当前簇 i 下的所有数据点
            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas) # 将当前簇 i 进行二分 kMeans 处理
            sseSplit = np.sum(splitClustAss[:,1]) # 将二分 kMeans 结果中的平方和的距离进行求和
            # 将未参与二分 kMeans 分配结果中的平方和的距离进行求和
            sseNotSplit = np.sum(clusterAssment[np.nonzero(clusterAssment[:,0].A!=i)[0],1]) 
            print('参与二分 K-Means 数据点的 SSE: ' ,sseSplit)
            print('未参与二分 K-Means 数据点的 SSE: ' ,sseNotSplit)
            # 总的（未拆分和已拆分）误差和越小，越相似，效果越优化，划分的结果更好
            if (sseSplit + sseNotSplit) < lowestSSE: 
                bestCentToSplit = i  # 在簇 i 上再进行 KMeans聚类 k=2
                bestNewCents = centroidMat # 当前最佳质心
                bestClustAss = splitClustAss.copy() # 当前最佳簇分配结果（距离和簇下标）
                lowestSSE = sseSplit + sseNotSplit
                
        # 更新簇的分配结果    
        # 调用二分 kMeans 的结果，默认簇是 0,1. 当然也可以改成其它的数字
        bestClustAss[np.nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList) 
        bestClustAss[np.nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit # 更新为最佳质心
        print('在簇 %d 上再进行 KMeans聚类（k = 2 )'%bestCentToSplit)
        print('簇分配结果:\n', bestClustAss)
        # 更新质心列表
        centList[bestCentToSplit] = bestNewCents[0,:].tolist()[0] # 更新原质心 list 中的第 i 个质心为使用二分 kMeans 后 bestNewCents 的第一个质心
        centList.append(bestNewCents[1,:].tolist()[0]) # 添加 bestNewCents 的第二个质心
        clusterAssment[np.nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss # 重新分配最好簇下的数据（质心）以及SSE
    return np.mat(centList), clusterAssment
```

🏃‍ 运行该代码：

<img src="https://gitee.com/veal98/images/raw/master/img/20200724221722.png" style="zoom:80%;" />

```python
centList,myNewAssments = biKMeans(dataMat2, 3) # 分 3 簇
```

<img src="https://gitee.com/veal98/images/raw/master/img/20200724221749.png" style="zoom:80%;" />

查看最终质心：

<img src="https://gitee.com/veal98/images/raw/master/img/20200724221912.png" style="zoom:80%;" />

簇分配示意图如下：

<img src="https://gitee.com/veal98/images/raw/master/img/20200724222003.png" style="zoom:80%;" />

## 📚 References

- 《Machine Learning in Action》

  <img src="https://gitee.com/veal98/images/raw/master/img/20200804111716.png" style="zoom:80%;" />

- [Github - AiLearning](https://github.com/apachecn/AiLearning/)