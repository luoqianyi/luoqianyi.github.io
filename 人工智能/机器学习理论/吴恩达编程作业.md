# 💻  吴恩达 Coursera 机器学习 — 编程作业 

## 🚀 Ex1：线性回归

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

### 1. 简单练习

输出一个 `5*5` 的单位矩阵

```python
np.eye(5)
```

![](https://gitee.com/veal98/images/raw/master/img/20200615153229.png)

### 2. 单变量线性回归

在本部分的练习中，您将使用一个变量实现线性回归，以预测食品卡车的利润。假设你是一家餐馆的首席执行官，正在**考虑不同的城市开设一个新的分店**。该连锁店已经在各个城市拥有卡车，而且你有来自城市的利润和人口数据。
您希望**使用这些数据来帮助您选择将哪个城市扩展到下一个城市**。

> 整个 2 的部分需要根据城市人口数量，预测开小吃店的利润 数据在 ex1 / ex1data1.txt 里，第一列是城市人口数量，第二列是该城市小吃店利润。
>
> 💡 **ex1data1.txt**：
>
> 6.1101,17.592
>
> 5.5277,9.1302
>
> 8.5186,13.662
>
> 7.0032,11.854
>
> 5.8598,6.8233
>
> 8.3829,11.886
>
> 7.4764,4.3483
>
> 8.5781,12

#### 2.1 展示数据

读入数据，然后展示数据

```python
path = 'ex1/ex1data1.txt'
data = pd.read_csv(path,names = ['Population','Profit']) # 读取数据并赋予列名
data.plot(kind='scatter', x='Population', y='Profit'，figsize=(8,5)) # scatter 表示散点图
```

![](https://gitee.com/veal98/images/raw/master/img/20200615155158.png)

#### 2.2 梯度下降

现在让我们使用梯度下降来实现线性回归，以最小化成本函数。 

- **公式**

  首先，我们将创建一个以参数θ为特征函数的代价函数

  ![](https://gitee.com/veal98/images/raw/master/img/20200615155938.png)

  计算代价函数 J(θ)：

  ```python
  def computeCost(X, y, theta):
      inner = np.power(((X*theta.T)-y),2)
      return np.sum(inner) / (2*len(X))
  ```

- **实现**

  数据前面已经读取完毕，我们要为加入一列 x，用于更新 θ0（即添加 $x_0 = 1$），然后我们将 θ 初始化为 0，学习率初始化为 0.01，迭代次数为 1500 次

  ```python
  data.insert(0, 'Ones', 1)
  ```

  ![](https://gitee.com/veal98/images/raw/master/img/20200615163040.png)

  现在我们来做一些变量初始化：

  ```python
  # 初始化 X 和 y
  cols = data.shape[1]  # 列数
  X = data.iloc[:,0:cols-1]  # 取前cols-1列，即输入向量 / 特征
  y = data.iloc[:,cols-1:cols] # 取最后一列，即输出向量
  ```

  ![](https://gitee.com/veal98/images/raw/master/img/20200615163506.png)

  ![](https://gitee.com/veal98/images/raw/master/img/20200615163453.png)

  

  代价函数是应该是numpy矩阵，所以我们需要转换X和Y，然后才能使用它们。 我们还需要初始化theta：

  ```python
  X = np.matrix(X.values)
  y = np.matrix(y.values)
  theta = np.matrix(np.array([0,0])) # theta 是一个(1,2)矩阵
  ```

  > 🚨 这里我使用的是 matix 而不是 array，两者基本通用。
  >
  > 但是matrix的优势就是相对简单的运算符号，比如两个矩阵相乘，就是用符号 `*`，但是array相乘不能这么用，得用方法 `.dot()`
  > array的优势就是不仅仅表示二维，还能表示3、4、5…维，而且在大部分Python程序里，array也是更常用的。
  >
  > 💭 两者区别：
  >
  > - 对应元素相乘：matrix 可以用 `np.multiply(X2,X1)`，array直接 `X1*X2`
  > - 点乘：matrix 直接 `X1*X2`，array可以 `X1@X2 `或 `X1.dot(X2)` 或 `np.dot(X1, X2)`

  ![](https://gitee.com/veal98/images/raw/master/img/20200615163752.png)

  看下维度，确保计算没问题：

  ```python
  X.shape, theta.shape, y.shape
  # ((97, 2), (1, 2), (97, 1))
  ```

- **计算J(θ)**

  计算代价函数 (theta初始值为0)，答案应该是32.07

  ```python
  computeCost(X, y, theta) # 32.072733877455676
  ```

- **梯度下降**

  ![](https://gitee.com/veal98/images/raw/master/img/20200615164558.png)

  > ⭐ 记住J(θ)的变量是θ，而不是X和y，意思是说，我们变化θ的值来使J(θ)变化，而不是变化X和y的值。 **一个检查梯度下降是不是在正常运作的方式，是打印出每一步J(θ)的值，看他是不是一直都在减小，并且最后收敛至一个稳定的值。** θ 最后的结果会用来预测小吃店在35000及70000人城市规模的利润。

  使用 **vectorization 向量化** 同时更新所有的 θ，可以大大提高效率

  ```python
  # 梯度下降
  def gradientDescent(X,y,theta,alpha,iters): # iters 表示迭代次数
      temp = np.matrix(np.zeros(theta.shape)) # 初始化 θ 的临时矩阵（1，2）
      parameters = int(theta.ravel().shape[1]) # 参数 θ 的数量
      cost = np.zeros(iters) # 初始化一个ndarray，包含每次迭代后代价函数的值
      m = X.shape[0] # 样本数量
      
      for i in range(iters):
          temp = theta - (alpha / m) * (X * theta.T - y).T * X
          theta = temp 
          cost[i] = computeCost(X,y,theta) # 输出每次迭代后代价函数的值
      return theta,cost
  ```

  初始化一些附加变量 - 学习速率α和要执行的迭代次数，2.2.2中已经提到：

  ```python
  alpha = 0.01
  iters = 1500
  ```

  现在让我们运行梯度下降算法来将我们的参数θ适合于训练集：

  ```python
  final_theta, cost = gradientDescent(X, y, theta, alpha, iters)
  ```

  ![](https://gitee.com/veal98/images/raw/master/img/20200615170111.png)

  最后，我们可以使用我们拟合的参数计算训练模型的代价函数（误差）：

  ```python
  computeCost(X, y, final_theta) # 4.483388256587726
  ```

  

  现在我们来绘制线性模型以及数据，直观地看出它的拟合。

  `np.linspace()` 在指定的间隔内返回均匀间隔的数字。

  ```python
  x = np.linspace(data.Population.min(), data.Population.max(), 100)  # 横坐标
  f = final_theta[0, 0] + (final_theta[0, 1] * x)  # 纵坐标，利润
  
  fig = plt.figure()
  ax = fig.add_subplot(1,1,1)
  ax.plot(x, f, 'r', label='Prediction')
  ax.scatter(data['Population'], data.Profit, label='Traning Data')
  ax.legend(loc=2)  # 2表示在左上角
  ax.set_xlabel('Population')
  ax.set_ylabel('Profit')
  ax.set_title('Predicted Profit vs. Population Size')
  ```

  <img src="https://gitee.com/veal98/images/raw/master/img/20200615171050.png" style="zoom:80%;" />

  由于梯度方程式函数在每个训练迭代中输出了代价函数的值，所以我们也可以绘制。 请注意，线性回归中的代价函数总是降低的 - 这是凸优化问题的一个例子。

  ```python
  fig, ax = plt.subplots(figsize=(6,4))
  ax.plot(np.arange(iters), cost, 'r')  # np.arange()返回等差数组
  ax.set_xlabel('Iterations')
  ax.set_ylabel('Cost')
  ax.set_title('Error vs. Training Epoch')
  ```

  <img src="https://gitee.com/veal98/images/raw/master/img/20200615172057.png" style="zoom:80%;" />



### 3. 多变量线性回归

ex1data2.txt 里的数据，第一列是房屋大小，第二列是卧室数量，第三列是房屋售价

**根据已有数据，建立模型，预测房屋的售价**

> 💡 **ex1data2.txt**
>
> 2104,3,399900
>
> 1600,3,329900
>
> 2400,3,369000
>
> 1416,2,232000
>
> 3000,4,539900
>
> 1985,4,299900
>
> 1534,3,314900
>
> 1427,3,198999
>
> 1380,3,212000

```python
data2 = pd.read_csv('ex1/ex1data2.txt', names=['Size','Bedrooms','Price'])
```

![](https://gitee.com/veal98/images/raw/master/img/20200615172627.png)



#### 3.1 特征归一化

观察数据发现，size变量是bedrooms变量的1000倍大小,**统一量级会让梯度下降收敛的更快**。做法就是**均值归一化**：将每类特征减去他的平均值后除以标准差

```python
data2 = (data2 - data2.mean()) / data2.std()
```

![](https://gitee.com/veal98/images/raw/master/img/20200615172845.png)

#### 3.2 梯度下降

现在我们重复单变量线性回归中的预处理步骤，并对新数据集运行线性回归程序。

```python
# add ones column
data2.insert(0, 'Ones', 1)

# set X (training data) and y (target variable)
cols = data2.shape[1]
X2 = data2.iloc[:,0:cols-1]
y2 = data2.iloc[:,cols-1:cols]

# convert to matrices and initialize theta
X2 = np.matrix(X2.values)
y2 = np.matrix(y2.values)
theta2 = np.matrix(np.array([0,0,0]))

# perform linear regression on the data set
g2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)

# get the cost (error) of the model
computeCost(X2, y2, g2), g2
```

![](https://gitee.com/veal98/images/raw/master/img/20200615173109.png)

查看 J(θ) 是否在不断减小并趋于固定值：

```python
fig, ax = plt.subplots(figsize=(6,4))
ax.plot(np.arange(iters), cost2, 'r')
ax.set_xlabel('Iterations')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')
```

![](https://gitee.com/veal98/images/raw/master/img/20200615173307.png)

#### 3.3 正规方程

正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial }{\partial {{\theta }_{j}}}J\left( {{\theta }_{j}} \right)=0$ 。 假设我们的训练集特征矩阵为 X（包含了x0=1）并且我们的训练集结果为向量 y，则利用正规方程解出向量 $\theta ={{\left( {{X}^{T}}X \right)}^{-1}}{{X}^{T}}y$。 上标T代表矩阵转置，上标-1 代表矩阵的逆。设矩阵 $A=X^TX$，则：${{\left( {{X}^{T}}X \right)}^{-1}}={{A}^{-1}}$

梯度下降与正规方程的比较：

- **梯度下降**：<u>需要选择学习率α，需要多次迭代</u>，当特征数量n大时也能较好适用，适用于各种类型的模型

- **正规方程**：不需要选择学习率α，一次计算得出，需要计算${{\left( {{X}^{T}}X \right)}^{-1}}$，如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为O(n^3)，通常来说当n小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型

```python
# 正规方程
def normalEqn(X, y):
    theta = np.linalg.inv(X.T@X)@X.T@y # np.linalg.inv 求逆；X.T@X等价于X.T.dot(X)
    return theta
```

```python
final_theta2=normalEqn(X, y) # 和批量梯度下降的theta的值有点差距
```

![](https://gitee.com/veal98/images/raw/master/img/20200615173928.png)

## 🚀 Ex2：逻辑回归 + 正则化

轻车熟路，先导包：

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

### 1. 逻辑回归

在这部分的练习中，**你将建立一个逻辑回归模型来预测一个学生是否能进入大学**。假设你是一所大学的行政管理人员，你想根据两门考试的结果，来决定每个申请人是否被录取。你有以前申请人的历史数据，可以将其用作逻辑回归训练集。对于每一个训练样本，**你有申请人两次测评的分数以及录取的结果**。为了完成这个预测任务，我们准备构建一个可以基于两次测试评分来评估录取可能性的分类模型。

部分数据如下：

> 4.62365962451697,78.0246928153624,0
>
> 30.28671076822607,43.89499752400101,0
>
> 35.84740876993872,72.90219802708364,0
>
> 60.18259938620976,86.30855209546826,1
>
> 79.0327360507101,75.3443764369103,1
>
> 45.08327747668339,56.3163717815305,0
>
> 61.10666453684766,96.51142588489624,1
>
> 75.02474556738889,46.55401354116538,1

#### ① 数据可视化

```python
data = pd.read_csv('ex2/ex2data1.txt',names = ['Exam1','Exam2','Admitted'])
data.head()
```

![](https://gitee.com/veal98/images/raw/master/img/20200618200748.png)

```python
data.describe()
```

![](https://gitee.com/veal98/images/raw/master/img/20200618200856.png)

让我们创建两个分数的散点图，并使用颜色编码来可视化，如果样本是正的（被接纳）或负的（未被接纳）

```python
positive = data[data.Admitted.isin(['1'])] # 1
negetive = data[data.Admitted.isin(['0'])] # 0

fig,ax = plt.subplots(figsize = (6,5))

# 设置 Admitter 为蓝色 o 形（默认）
ax.scatter(positive['Exam1'],positive['Exam2'],c = 'b',label = 'Admitted')
# 设置 Not Admitted 为红色 x 形
ax.scatter(negetive['Exam1'], negetive['Exam2'], s=50, c='r', marker='x', label='Not Admitted')

# 设置图例显示在图的上方
box = ax.get_position()
ax.set_position([box.x0, box.y0, box.width , box.height* 0.8])
ax.legend(loc='center left', bbox_to_anchor=(0.2, 1.12),ncol=3)

# 设置横纵坐标名
ax.set_xlabel('Exam 1 Score')
ax.set_ylabel('Exam 2 Score')

plt.show()
```

![](https://gitee.com/veal98/images/raw/master/img/20200618202024.png)

看起来在两类间，有一个清晰的决策边界。现在我们需要实现逻辑回归，那样就可以训练一个模型来预测结果。

> 💡 `plt.scatter` 详解：
>
> ![](https://gitee.com/veal98/images/raw/master/img/20200618201411.png)

#### ② 假设函数 Sigmoid

首先来回顾下 logistic回归的假设函数：<img src="https://gitee.com/veal98/images/raw/master/img/20200618202129.png" style="zoom: 67%;" />

其中的 g 代表一个常用的logistic function为S形函数（Sigmoid function）：<img src="https://gitee.com/veal98/images/raw/master/img/20200618202152.png" style="zoom: 67%;" />

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

让我们做一个快速的检查，来确保它可以工作

```python
x1 = np.arange(-10,10,0.1)
plt.plot(x1,sigmoid(x1),c = 'r')
plt.show()
```

![](https://gitee.com/veal98/images/raw/master/img/20200618202353.png)

#### ③ 代价函数

逻辑回归的代价函数如下，和线性回归的代价函数不一样，因为这个函数是凸的。

<img src="https://gitee.com/veal98/images/raw/master/img/20200618202440.png" style="zoom: 67%;" />

```python
def cost(theat, X, y):
    first = (-y) * np.log(sigmoid(X @ theta))
    second = (1-y) * np.log(1-sigmoid(X @ theta))
    return np.sum(first - second) / (len(X))
```

现在，我们要做一些设置，获取我们的训练集数据。

```python
# 加一列常数列
data.insert(0, 'Ones', 1)

# 初始化X，y，θ
cols = data.shape[1] # 获取列数
X = data.iloc[:,0:cols-1]
y = data.iloc[:,cols-1:cols]
theta = np.zeros(X.shape[1])

# 转换X，y的类型
X = np.array(X.values)
y = np.array(y.values)
```

让我们来检查矩阵的维度，确保一切良好。

![](https://gitee.com/veal98/images/raw/master/img/20200618203843.png)

运行代价函数：

```python
cost(theta,X,y) # 0.6931471805599453
# 代价函数的返回值即代价函数的最小值
```

#### ④ 梯度下降

<img src="https://gitee.com/veal98/images/raw/master/img/20200618204308.png" style="zoom: 80%;" />

```python
# 实现梯度计算的函数（但是仅仅计算出了θ，并没有更新θ）
def gradient(theta, X, y):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    
    parameters = int(theta.ravel().shape[1])
    grad = np.zeros(parameters)
    
    error = sigmoid(X * theta.T) - y
    
    for i in range(parameters):
        term = np.multiply(error, X[:,i])
        grad[i] = np.sum(term) / len(X) # 计算出3个θ值
    
    return grad 
```

```python
gradient(theta,X,y) # array([ -0.1, -12.00921659, -11.26284221])
```

> 💡 `numpy.ravel` 函数详解：
>
> **此函数返回展平的一维数组**。只在需要时才制作副本。返回的数组将与输入数组的类型相同。该函数有一个参数。
>
> ```python
> numpy.ravel(a, order)
> ```
>
> 构造函数采用以下参数。
>
> | 序号 | 参数和描述                                                   |
> | :--- | :----------------------------------------------------------- |
> | 1    | **order** 'C'：行主要(默认。'F'：列主要'A'：按列列主要顺序展平，如果a是Fortran在内存中连续，则行主要顺序否则'K'：按顺序压扁a发生在记忆中 |

#### ⑤ 用工具库计算θ的值

注意，**我们实际上没有在这个函数中执行梯度下降，我们仅仅在计算梯度。**在练习中，吴恩达老师使用的是一个称为“`fminunc`”的`Octave`函数是用来优化函数来计算成本和梯度参数。由于我们使用Python，我们可以用 `scipy.optimize.fmin_tnc` 来做同样的事情。

这里我们使用的是高级优化算法，运行速度通常远远超过梯度下降。方便快捷。
只需传入 cost 函数，已经所求的变量 theta，和梯度。cost 函数定义变量时变量 tehta 要放在第一个，若 cost 函数只返回 cost，则设置`fprime=gradient`。

```python
import scipy.optimize as opt
result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))
result
# (array([-25.16131867,   0.20623159,   0.20147149]), 36, 0)
```

#### ⑥ 评估逻辑回归模型

学习好了参数θ后，我们来用这个模型预测某个学生是否能被录取。

接下来，我们需要编写一个函数，用我们所学的参数theta来为数据集X输出预测。然后，我们可以使用这个函数来给我们的分类器的训练精度打分。

逻辑回归模型的假设函数：<img src="https://gitee.com/veal98/images/raw/master/img/20200618210313.png" style="zoom:67%;" />

- 当 hθ 大于等于 0.5 时，预测 y = 1

- 当 hθ 小于 0.5 时，预测 y = 0 

```python
def predict(theta, X):
    probability = sigmoid(X@theta) 
    return [1 if x >= 0.5 else 0 for x in probability]  # return a list

final_theta = result[0] # 将我们的算法结果与工具库中的算法进行比较
predictions = predict(final_theta,X)
correct = [1 if a==b else 0 for (a, b) in zip(predictions, y)]
accuracy = sum(correct) / len(X)
accuracy
```

![](https://gitee.com/veal98/images/raw/master/img/20200618211219.png)

可以看到我们预测精度达到了89%，not bad.

#### ⑦ 决策边界

<img src="https://gitee.com/veal98/images/raw/master/img/20200618212931.png" style="zoom:80%;" />

画出决策边界：

```python
x1 = np.arange(130, step=0.1)
x2 = ( - result[0][0] - result[0][1] * x1) / result[0][2]

fig, ax = plt.subplots(figsize=(8,5))
ax.scatter(positive['exam1'], positive['exam2'], c='b', label='Admitted')
ax.scatter(negetive['exam1'], negetive['exam2'], s=50, c='r', marker='x', label='Not Admitted')
ax.plot(x1, x2) # 画出决策边界
ax.set_xlim(0, 130)
ax.set_ylim(0, 130)
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_title('Decision Boundary')
plt.show()
```

![](https://gitee.com/veal98/images/raw/master/img/20200618212101.png)

### 2. 正则化逻辑回归

在训练的第二部分，我们将要通过加入正则项提升逻辑回归算法。简而言之，正则化是成本函数中的一个术语，它使算法更倾向于“更简单”的模型（在这种情况下，模型将更小的系数）。这个理论助于减少过拟合，提高模型的泛化能力。

设想你是工厂的生产主管，你有一些芯片在两次测试中的测试结果。对于这两次测试，你想决定是否芯片要被接受或抛弃。为了帮助你做出艰难的决定，你拥有过去芯片的测试数据集，从其中你可以构建一个逻辑回归模型。

部分数据如下：

> 0.051267,0.69956,1
>
> -0.092742,0.68494,1
>
> -0.21371,0.69225,1
>
> -0.375,0.50219,1
>
> -0.51325,0.46564,1
>
> -0.52477,0.2098,1
>
> -0.39804,0.034357,1
>
> -0.30588,-0.19225,1

#### ① 数据可视化

```python
data_init = pd.read_csv('ex2/ex2data2.txt', names=['Test1', 'Test2', 'Accepted'])
data_init.head()
```

![](https://gitee.com/veal98/images/raw/master/img/20200620102339.png)

接下里用散点图绘制数据：

```python
def plot_data():
    positive = data_init[data_init['Accepted'].isin([1])]
    negative = data_init[dadata_initta2['Accepted'].isin([0])]
    
    fig,ax = plt.subplots(figsize = (8,5))
    ax.scatter(positive['Test1'],positive['Test2'], s = 50, c = 'b', marker = 'o', label = 'Accepted')
    ax.scatter(negative['Test1'],negative['Test2'], s = 50, c = 'r', marker = 'x', label = 'Rejected')
    ax.legend()
    ax.set_xlabel('Test1 Score')
    ax.set_ylabel('Test2 Score')

plot_data()
```

![](https://gitee.com/veal98/images/raw/master/img/20200620100814.png)

以上图片显示，这个数据集不能像之前一样使用直线将两部分分割。而逻辑回归只适用于线性的分割，所以，这个数据集不适合直接使用逻辑回归。

#### ② 特征映射 Feature mapping

>  💡 首先解释一下**什么是特征映射**：
>
>  如果样本量多，逻辑回归问题很复杂，而原始特征只有 x1, x2，可以用多项式创建更多的特征x1、x2、x1x2、x1^2、x2^2、... X1^nX2^n。因为更多的特征进行逻辑回归时，得到的分割线可以是任意高阶函数的形状。

一个拟合数据的更好的方法是从每个数据点创建更多的特征。

我们将把这些特征映射到所有的 x1 和 x2 的多项式项上，直到第六次幂。

![](https://gitee.com/veal98/images/raw/master/img/20200620102006.png)



```python
def feature_mapping(x1, x2, power):
    data = {}
    for i in np.arange(power + 1):
        for p in np.arange(i + 1):
            data["f{}{}".format(i - p, p)] = np.power(x1, i - p) * np.power(x2, p)

    return pd.DataFrame(data)

x1 = data_init['Test1'].values
x2 = data_init['Test2'].values

data = feature_mapping(x1, x2, power=6)
data.head()
```

![](https://gitee.com/veal98/images/raw/master/img/20200620120315.png)

经过映射，我们将有两个特征的向量转化成了一个 28 维的向量。

在这个高维特征向量上训练的logistic回归分类器将会有一个更复杂的决策边界，当我们在二维图中绘制时，会出现非线性。

虽**然特征映射允许我们构建一个更有表现力的分类器，但它也更容易过拟合**。在接下来的练习中，我们将实现正则化的logistic回归来拟合数据，并且可以看到正则化如何帮助解决过拟合的问题。

#### ③ 代价函数

正则化逻辑回归模型的代价函数如下：

![](https://gitee.com/veal98/images/raw/master/img/20200603215306.png)

注意：θ0 是不需要正则化的，下标从 1 开始

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    # 正则项
    reg = (learningRate / (2 * len(X))) * np.sum(np.power(theta[:,1:theta.shape[1]], 2))
    return np.sum(first - second) / len(X) + reg
```

```python
# 初始化X，y，θ
cols = data.shape[1]
X = data.iloc[:,1:cols]
y = data.iloc[:,0:1]
theta = np.zeros(cols - 1) # θ0 是不需要正则化的
```

```python
# 类型转换
X = np.array(X.values)
y = np.array(y.values)

# λ设为1
learningRate = 1
```

计算初始代价：

```python
# 计算初始代价
costReg(theta, X, y, learningRate)
```

![](https://gitee.com/veal98/images/raw/master/img/20200620104950.png)

#### ④ 梯度下降

因为我们未对 θ0 进行正则化，所以梯度下降算法将分两种情形：

![](https://gitee.com/veal98/images/raw/master/img/20200603215413.png)

对上面的算法中 j = 1,2,3...n 时的更新式子进行调整可得：

<img src="https://gitee.com/veal98/images/raw/master/img/20200603214135.png" style="zoom: 80%;" />



```python
# 逻辑回归梯度下降
def gradient(theta, X, y):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    
    parameters = int(theta.ravel().shape[1])
    grad = np.zeros(parameters)
    
    error = sigmoid(X * theta.T) - y
    
    for i in range(parameters):
        term = np.multiply(error, X[:,i])
        grad[i] = np.sum(term) / len(X) # 计算出3个θ值
    
    return grad 

# 正则化逻辑回归梯度下降
def gradientReg(theta, X, y, learningRate):
    reg = (learningRate / len(X)) * theta
    reg[0] = 0
    return gradient(theta, X, y) + reg
```

![](https://gitee.com/veal98/images/raw/master/img/20200620112828.png)

#### ⑤ 用工具库求解参数

使用 `scipy.optimize.fmin_tnc` 计算代价函数和梯度

```python
import scipy.optimize as opt
result = opt.fmin_tnc(func=costReg, x0=theta, fprime=gradientReg, args=(X, y, learningRate))

result
```

![](https://gitee.com/veal98/images/raw/master/img/20200620113045.png)

#### ⑥ 评估正则化逻辑回归模型

同样，使用上一节实验中的评估函数查看我们的准确度

```python
def predict(theta, X):
    probability = sigmoid(X@theta) 
    return [1 if x >= 0.5 else 0 for x in probability]  # return a list

final_theta = result[0] # 将我们的算法结果与工具库中的算法进行比较
predictions = predict(final_theta,X)
correct = [1 if a==b else 0 for (a, b) in zip(predictions, y)]
accuracy = sum(correct) / len(correct)
accuracy
```

![](https://gitee.com/veal98/images/raw/master/img/20200620113624.png)

#### ⑦ 决策边界

> ✅ Todo

## 🚀 Ex3：多类别逻辑回归 + 神经网络

### 1. 多类分类

这个部分需要实现手写数字（0到9）的识别。我们将扩展我们在上一节编程作业中写的 logistic 回归的实现，并将其应用于一对多的分类（不止两个类别）。

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.io import loadmat
```

#### ① 数据集 DataSet

首先，加载数据集。这里的数据为MATLAB的格式，所以要使用`SciPy.io`的`loadmat`函数。

这个 MATLAB 格式的 `.mat` 文件，包含 5000 个 20*20 像素的手写字体图像，以及他对应的数字。另外，数字0 的 y 值，对应的是10

```python
def load_data(path):
    data = loadmat(path)
    X = data['X']
    y = data['y']
    return X,y

X,y = load_data('ex3/ex3data1.mat')
print(np.unique(y))  # 看下有几类标签 [ 1  2  3  4  5  6  7  8  9 10]
X.shape, y.shape  # (5000, 400), (5000, 1))
```

其中有5000个训练样本，每个样本是20*20像素的数字的灰度图像。每个像素代表一个浮点数，表示该位置的灰度强度。20×20 的像素网格被展开成一个 400 维的向量。在我们的数据矩阵X中，每一个样本都变成了一行，这给了我们一个 5000×400 矩阵 X，每一行都是一个手写数字图像的训练样本。

<img src="https://gitee.com/veal98/images/raw/master/img/20200621114035.png" style="zoom:80%;" />

#### ② 数据可视化

```python
def plot_an_image(X):
    # 随机打印一个数字
    pick_one = np.random.randint(0,5000)
    image = X[pick_one,:]
    fig,ax = plt.subplots(figsize = (1,1))
    ax.matshow(image.reshape((20,20)),cmap = 'gray_r')
    plt.show()
    print('this should be {}'.format(y[pick_one]))

plot_an_image(X)
```

![](https://gitee.com/veal98/images/raw/master/img/20200621114530.png)

```python
def plot_100_image(X):
    # 随机找出 100 张图片
    sample_idx =np.random.choice(np.arange(X.shape[0]),100) # 随机选 100 个样本
    sample_images = X[sample_idx,:] # (100,400)
    fig,ax_array = plt.subplots(nrows = 10, ncols = 10, sharey = True, sharex = True, figsize = (8,8))
    
    for row in range(10):
        for column in range(10):
            ax_array[row,column].matshow(sample_images[10*row + column].reshape((20,20)),cmap = 'gray_r')
          
    plt.xticks([]) # 去除刻度，美观
    plt.yticks([])        
    plt.show()
    
plot_100_image(X)
```

![](https://gitee.com/veal98/images/raw/master/img/20200621115348.png)

#### ③ 向量化逻辑回归 Vectorizing

你将用多分类逻辑回归做一个分类器。因为现在有10个数字类别，所以你需要训练10个不同的逻辑回归分类器。为了让训练效率更高，**将逻辑回归向量化**是非常重要的，不要用循环。

<u>在本节中，我们将实现一个不使用任何for循环的向量化的logistic回归版本</u>。

##### Ⅰ 向量化代价函数

首先写出向量化的代价函数。

回想正则化的logistic回归的代价函数是：

![](https://gitee.com/veal98/images/raw/master/img/20200621115706.png)

事实上我们可以对所有的样本用矩阵乘法来快速的计算。让我们如下来定义 X 和 θ ：

<img src="https://gitee.com/veal98/images/raw/master/img/20200621115845.png" style="zoom:80%;" />

🚩 如果 a 和 b 都是向量，那么 $a^Tb = b^Ta$，则：

<img src="https://gitee.com/veal98/images/raw/master/img/20200621120015.png" style="zoom:80%;" />

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def regularized_cost(theta, X, y, l):
    """
    args:
        X: feature matrix, (m, n+1) # 插入了x0=1
        y: target vector, (m, )
        l: lambda constant for regularization
    """
    thetaReg = theta[1:] # theta_0 不需要正则化
    first = (-y*np.log(sigmoid(X@theta))) + (y-1)*np.log(1-sigmoid(X@theta))
    reg = (thetaReg@thetaReg)*l / (2*len(X))
    return np.mean(first) + reg
```

##### Ⅱ 向量化梯度

回顾正则化logistic回归代价函数的梯度下降法如下表示，因为不惩罚 $\theta_0$，所以分为两种情况：

![](https://gitee.com/veal98/images/raw/master/img/20200603215413.png)

令 $(h_\theta(x^{(i)})-y^{(i)}) = β_i$  同理：

<img src="https://gitee.com/veal98/images/raw/master/img/20200621141545.png" style="zoom:80%;" />

所以其中的梯度表示如下：

```python
def regularized_gradient(theta, X, y, l):
    """
    don't penalize theta_0
    args:
        l: lambda constant
    return:
        a vector of gradient
    """
    thetaReg = theta[1:]
    first = (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y)
    
    # 这里人为插入一维0，使得对theta_0不惩罚，方便计算
    reg = np.concatenate([np.array([0]), (l / len(X)) * thetaReg])
    return first + reg
```

> 💡 `concatenate((a1, a2, …), axis=0)` 数组拼接函数 
>
> 参数: 
>
> - a1, a2 ……为要拼接的数组 
> - axis 为在哪个维度上进行拼接，默认为 0

#### ④ 一对多分类器 One-vs-all Classification

现在我们已经定义了代价函数和梯度函数，现在是构建分类器的时候了。 对于这个任务，我们有10个可能的类，并且由于逻辑回归只能一次在 2 个类之间进行分类，我们需要多类分类的策略。 在本练习中，我们的任务是实现一对一全分类方法，其中**具有 k 个不同类的标签就有 k 个分类器，每个分类器在 “类别 i” 和 “不是 i” 之间决定**。 我们将把分类器训练包含在一个函数中，该函数计算 10 个分类器中的每个分类器的最终权重，并将权重返回为 `k * (n + 1)` 数组，其中 n 是参数数量。

这里需要注意的几点：

- 首先，我们为X添加了一列常数项 1 ，以计算截距项（常数项）。 
- 其次，我们将 y 从类标签转换为每个分类器的二进制值（要么是类 i，要么不是类 i）。 
- 最后，我们使用 `SciPy` 的较新优化API来最小化每个分类器的代价函数。 如果指定的话，API 将采用目标函数，初始参数集，优化方法和`jacobian`（渐变）函数。 然后将优化程序找到的参数分配给参数数组。

```python
from scipy.optimize import minimize

def one_vs_all(X, y, l, K):
    """generalized logistic regression
    args:
        X: feature matrix, (m, n+1) # with incercept x0=1
        y: target vector, (m, )
        l: lambda constant for regularization
        K: numbel of labels
    return: trained parameters
    """
    all_theta = np.zeros((K, X.shape[1]))  # (10, 401)
    
    for i in range(1, K+1):
        theta = np.zeros(X.shape[1])
        y_i = np.array([1 if label == i else 0 for label in y])
    
        ret = minimize(fun=regularized_cost, x0=theta, args=(X, y_i, l), method='TNC',
                        jac=regularized_gradient, options={'disp': True})
        all_theta[i-1,:] = ret.x
                         
    return all_theta
```



> 💡 `scipy.optimize.minimize`函数详解：(`minimize` 是局部最优的解法 )
>
> ```python
> scipy.optimize.minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None,  callback=None, options=None)
> ```
>
> - `fun `:  求最小值的目标函数 
>
> - `x0`: 变量的初始猜测值，如果有多个变量，需要给每个变量一个初始猜测值。
>
> - `args`: 常数值，fun 中没有数字，都以变量的形式表示，对于常数项，需要在这里给值 
>
> - `method`: 求极值的方法，官方文档给了很多种。一般使用默认 
>
>   ![](https://gitee.com/veal98/images/raw/master/img/20200621143625.png)
>
> - `jac`：渐变函数
>
> - `constraints `: 约束条件，针对fun中为参数的部分进行约束限制

实现向量化代码的一个更具挑战性的部分是正确地写入所有的矩阵，保证维度正确。

```python
def predict_all(X, all_theta):
    # compute the class probability for each class on each training instance   
    h = sigmoid(X @ all_theta.T)  # 注意的这里的all_theta需要转置
    
    # create array of the index with the maximum probability
    # Returns the indices of the maximum values along an axis.
    h_argmax = np.argmax(h, axis=1)
    
    # because our array was zero-indexed we need to add one for the true label prediction
    h_argmax = h_argmax + 1
    
    return h_argmax
```

这里的`h`共5000行，10列，每行代表一个样本，每列是预测对应数字的概率。我**们取概率最大对应的`index`加 1 就是我们分类器最终预测出来的类别**。返回的`h_argmax`是一个array，包含5000个样本对应的预测值。

```python
raw_X, raw_y = load_data('ex3data1.mat')
X = np.insert(raw_X, 0, 1, axis=1) # (5000, 401)
y = raw_y.flatten()  # 这里消除了一个维度变成一维数组，方便后面的计算 or .reshape(-1) （5000，）


all_theta = one_vs_all(X, y, 1, 10)
all_theta  # 每一行是一个分类器的一组参数
```

> 💡 `numpy.ndarray.flatten` 函数：
>
> 返回一个折叠成一维的数组。但是该函数只能适用于numpy对象，即array或者mat，普通的list列表是不行的。

![](https://gitee.com/veal98/images/raw/master/img/20200621151149.png)

```python
y_pred = predict_all(X, all_theta)
accuracy = np.mean(y_pred == y)
print ('accuracy = {0}%'.format(accuracy * 100))
```

![](https://gitee.com/veal98/images/raw/master/img/20200621151214.png)

### 2. 神经网络

上面使用了多类logistic回归，然而logistic回归不能形成更复杂的假设，因为它只是一个线性分类器。

接下来我们用神经网络来尝试下，神经网络可以实现非常复杂的非线性的模型。我们将利用已经训练好了的权重进行预测。

#### ① 模型表达

<img src="https://gitee.com/veal98/images/raw/master/img/20200621151250.png" style="zoom:80%;" />

输入是图片的像素值，20*20像素的图片有400个输入层单元，不包括需要额外添加的加上常数项。 材料已经提供了训练好的神经网络的参数Θ(1),Θ(2)，有25个隐层单元和10个输出单元（10个输出）

#### ② 前馈神经网络和预测

你需要实现前馈神经网络预测手写数字的功能。和之前的一对多分类一样，神经网络的预测会把$(h_\theta(x))_k$中值最大的，作为预测输出

```python
def load_weight(path):
    data = loadmat(path)
    return data['Theta1'], data['Theta2']

theta1, theta2 = load_weight('ex3/ex3weights.mat')
theta1.shape, theta2.shape # ((25, 401), (10, 26))
```

插入常数项

```python
X, y = load_data('ex3/ex3data1.mat')
y = y.flatten()  # 将 y 转成一维数组，便于计算
X = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1)  # 插入常数项

X.shape, y.shape # ((5000, 401), (5000,))
```

按照上面的模型表示：

```python
a1 = X
z2 = a1 @ theta1.T
z2.shape
```

```python
z2 = np.insert(z2, 0, 1, axis=1)
z2.shape # (5000, 26)
```

```python
a2 = sigmoid(z2)
a2.shape # (5000, 26)
```

```python
z3 = a2 @ theta2.T
z3.shape # (5000, 10)
```

```python
a3 = sigmoid(z3)
a3.shape # (5000, 10)
```

评估准确度：

```python
y_pred = np.argmax(a3, axis=1) + 1 
accuracy = np.mean(y_pred == y)
print ('accuracy = {0}%'.format(accuracy * 100))  # accuracy = 97.52%
```

![](https://gitee.com/veal98/images/raw/master/img/20200621152626.png)

虽然人工神经网络是非常强大的模型，但训练数据的准确性有可能无法完美预测实际数据，很容易过拟合。

## 🚀 Ex4：反向传播神经网络

### 1. 神经网络

在这个练习中，你将实现**反向传播算法来学习神经网络的参数**。依旧是上次预测手写数数字的例子。

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
```

#### ① 可视化数据

这部分我们随机选取100个样本并可视化。训练集共有5000个训练样本，每个样本是20*20像素的数字的灰度图像。每个像素代表一个浮点数，表示该位置的灰度强度。20×20的像素网格被展开成一个400维的向量。在我们的数据矩阵X中，每一个样本都变成了一行，这给了我们一个 5000×400 矩阵X，每一行都是一个手写数字图像的训练样本。

```python
def load_mat(path):
    # 读取数据
    data = loadmat(path)
    X = data['X']
    y = data['y'].flatten()
    return X,y
```

```python
def plot_100_images(X):
    """随机画100个数字"""
    index = np.random.choice(range(5000), 100)
    images = X[index]
    fig, ax_array = plt.subplots(10, 10, sharey=True, sharex=True, figsize=(8, 8))
    for r in range(10):
        for c in range(10):
            ax_array[r, c].matshow(images[r*10 + c].reshape(20,20), cmap='gray_r')
    plt.xticks([])
    plt.yticks([])
    plt.show()
```

```python
X,y = load_mat('ex4/ex4data1.mat')
plot_100_images(X)
```

![](https://gitee.com/veal98/images/raw/master/img/20200622144244.png)

#### ② 模型展示

我们的网络有三层，输入层，隐藏层，输出层。我们的输入是数字图像的像素值，因为每个数字的图像大小为20*20，所以我们输入层有400个单元（这里不包括总是输出要加一个偏置单元）。

![](https://gitee.com/veal98/images/raw/master/img/20200622144522.png)

##### Ⅰ 读取数据

首先我们要**将标签值（1，2，3，4，…，10）转化成非线性相关的向量**，向量对应位置（`y[i-1]`）上的值等于1，例如 `y[0]=6` 转化为 `y[0]=[0,0,0,0,0,1,0,0,0,0]`。

```python
def expand_y(y):
    result = []
    
    for i in y:
        y_array = np.zeros(10)
        y_array[i-1] = 1
        result.append(y_array)
        
    return np.array(result)
```

获取训练数据集，以及对训练集做相应的处理，得到我们的 input X，lables y：

```python
raw_X, raw_y = load_mat('ex4/ex4data1.mat')
X = np.insert(raw_X, 0, 1, axis = 1) # 插入偏置项 1
y = expand_y(raw_y) # 将标签值转换成向量
X.shape,y.shape # ((5000, 401), (5000, 10))
```

##### Ⅱ 读取参数

`ex4weights.mat` 文件中已经提供了训练好的参数 θ1 和 θ2。这些参数的维度由神经网络的大小决定，第二层有 25 个单元，输出层有 10 个单元(对应10个数字类/标签)。

```python
def load_weight(path):
    data = loadmat(path)
    return data['Theta1'],data['Theta2']
```

```python
t1,t2 = load_weight('ex4/ex4weights.mat')
t1.shape,t2.shape # ((25, 401), (10, 26))
```

##### Ⅲ 展开参数

当我们使用高级优化方法来优化神经网络时，我们需要**将多个参数矩阵展开成向量**，才能传入优化函数，然后再恢复形状。

```python
def serialize(a,b):
    # 展开参数
    return np.r_[a.flatten(),b.flatten()]

theta = serialize(t1,t2) # 扁平化参数，25*401+10*26=10285
theta.shape # (10285,)
```

```python
def deserialize(seq):
    '''提取参数'''
    return seq[:25*401].reshape(25, 401), seq[25*401:].reshape(10, 26)
```

> 💡 `numpy.r_`：按列连接两个矩阵。就是把两矩阵上下相加，要求列数相等。
>
> `np.c_`：按行连接两个矩阵。就是把两矩阵左右相加，要求行数相等。
>
> 举例如下：
>
> ```python
> a = np.array([[1, 2, 3],[7,8,9]])
> 
> b=np.array([[4,5,6],[1,2,3]])
> 
> a
> Out[4]: 
> array([[1, 2, 3],
>     [7, 8, 9]])
> 
> b
> Out[5]: 
> array([[4, 5, 6],
>     [1, 2, 3]])
> 
> c=np.c_[a,b]
> 
> c
> Out[7]: 
> array([[1, 2, 3, 4, 5, 6],
>     [7, 8, 9, 1, 2, 3]])
> 
> 
> 
> d= np.array([7,8,9])
> 
> e=np.array([1, 2, 3])
> 
> f=np.c_[d,e]
> 
> f
> Out[12]: 
> array([[7, 1],
>     [8, 2],
>     [9, 3]])
> ```

#### ③ 前馈和代价函数 Feedforward and cost function

##### Ⅰ 前馈

确保每层的单元数，注意输出时加一个偏置单元，s(1)=400+1，s(2)=25+1，s(3)=10。

![](https://gitee.com/veal98/images/raw/master/img/20200622150846.png)

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

```python
def feed_forward(theta,X,):
    '''得到每层的输入和输出'''
    t1, t2 = deserialize(theta)
    # 前面已经插入过偏置单元，这里就不用插入了
    a1 = X
    z2 = a1 @ t1.T
    a2 = np.insert(sigmoid(z2),0,1,axis = 1)
    z3 = a2 @ t2.T
    a3 = sigmoid(z3)
    
    return a1,z2,a2,z3,a3
```

```python
a1, z2, a2, z3, h = feed_forward(theta, X)
```

![](https://gitee.com/veal98/images/raw/master/img/20200622151351.png)

##### Ⅱ 代价函数

回顾下神经网络的代价函数（不带正则化项）

![](https://gitee.com/veal98/images/raw/master/img/20200622151435.png)

输出层输出的是对样本的预测，包含5000个数据，每个数据对应了一个包含10个元素的向量，代表了结果有10类。在公式中，每个元素与log项对应相乘。

最后我们使用提供训练好的参数θ，算出的cost应该为0.287629

```python
def cost(theta, X, y):
    a1, z2, a2, z3, h = feed_forward(theta, X)
    J = 0
    for i in range(len(X)):
        first = -y[i] * np.log(h[i])
        second = (1 - y[i]) * np.log(1 - h[i])
        J = J + np.sum(first - second)
    J = J / len(X)
    return J
```

![](https://gitee.com/veal98/images/raw/master/img/20200622151942.png)

#### ④ 正则化代价函数

![](https://gitee.com/veal98/images/raw/master/img/20200622152204.png)



**注意不要将每层的偏置项正则化。**

```python
def regularized_cost(theta, X, y, l = 1):
    '''正则化时忽略每层的偏置项，也就是参数矩阵的第一列'''
    t1,t2 = deserialize(theta)
    reg = np.sum(t1[:,1:] ** 2) + np.sum(t2[:,1:] ** 2)
    return 1 / (2 * len(X)) * reg + cost(theta, X, y)
```

![](https://gitee.com/veal98/images/raw/master/img/20200622152749.png)

### 2. 反向传播算法

#### ① sigmoid 梯度

你需要实现sigmoid函数的梯度（导数），公式如下：

![](https://gitee.com/veal98/images/raw/master/img/20200622153858.png)

```python
def sigmoid_gradient(z):
    return sigmoid(z) * (1 - sigmoid(z))
```

#### ② 随机初始化 Random initialization

当我们训练神经网络时，随机初始化参数是很重要的，可以打破数据的对称性。一个有效的策略是在均匀分布**(−e，e)**中随机选择值，我们可以选择 **e = 0.12** 这个范围的值来确保参数足够小，使得训练更有效率。

```python
def random_init(size):
    '''从服从的均匀分布的范围中随机返回size大小的值'''
    return np.random.uniform(-0.12,0.12, size)
```

#### ③ 反向传播 Backpropagation

![](https://gitee.com/veal98/images/raw/master/img/20200622155034.png)

![](https://gitee.com/veal98/images/raw/master/img/20200622154115.png)

目标：获取整个网络代价函数的梯度。以便在优化算法中求解。

首先明确各个参数的维度：

```python
print('a1', a1.shape,'t1', t1.shape)
print('z2', z2.shape)
print('a2', a2.shape, 't2', t2.shape)
print('z3', z3.shape)
print('a3', h.shape)
```

![](https://gitee.com/veal98/images/raw/master/img/20200622154316.png)

```python
def gradient(theta, X, y):
    t1,t2 = deserialize(theta)
    a1, z2, a2, z3, h = feed_forward(theta, X)
    d3 = h - y # 实际输出和预计输出的误差
    d2 = d3 @ t2[:,1:] * sigmoid_gradient(z2)
    D2 = d3.T @ a2
    D1 = d2.T @ a1
    D = (1 / len(X)) * serialize(D1,D2)
    
    return D
```

#### ④ 梯度校验

![](https://gitee.com/veal98/images/raw/master/img/20200622155301.png)

<img src="https://gitee.com/veal98/images/raw/master/img/20200622155723.png" style="zoom:80%;" />

如果你的反向传播计算正确，那你得出的这个数字应该**小于10e-9**

```python
def gradient_checking(theta, X, y, e):
    def a_numeric_grad(plus, minus):
        """
        对每个参数theta_i计算数值梯度，即理论梯度。
        """
        return (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (e * 2)
   
    numeric_grad = [] 
    for i in range(len(theta)):
        plus = theta.copy()  # deep copy otherwise you will change the raw theta
        minus = theta.copy()
        plus[i] = plus[i] + e
        minus[i] = minus[i] - e
        grad_i = a_numeric_grad(plus, minus)
        numeric_grad.append(grad_i)
    
    numeric_grad = np.array(numeric_grad)
    analytic_grad = regularized_gradient(theta, X, y)
    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)
 
gradient_checking(theta, X, y, epsilon= 0.0001) # 这个运行很慢，谨慎运行
```

#### ⑤ 正则化神经网络

梯度下降中加入正则项：

<img src="https://gitee.com/veal98/images/raw/master/img/20200622160921.png" style="zoom:80%;" />

其中：$△_{ij}^{(l)}$ 表示误差矩阵

```python
def regularized_gradient(theta,X, y, l = 1):
    """不惩罚偏置单元的参数"""
    a1, z2, a2, z3, h = feed_forward(theta, X)
    D1,D2 = deserialize(gradient(theta,X,y)) # D1,D2 表示非正则化计算出来的梯度
    t1,t2 = deserialize(theta)
    t1[:,0] = 0
    t2[:,0] = 0
    reg_D1 = D1 + (l / len(X)) * t1
    reg_D2 = D2 + (l / len(X)) * t2
    
    return serialize(reg_D1,reg_D2)
```

#### ⑥ 使用工具库计算参数最优解

```python
import scipy.optimize as opt
from scipy.optimize import minimize

def nn_training(X, y):
    init_theta = random_init(10285)  # 25*401 + 10*26

    res = opt.minimize(fun=regularized_cost,
                       x0=init_theta,
                       args=(X, y, 1),
                       method='TNC',
                       jac=regularized_gradient,
                       options={'maxiter': 400})
    return res
```

```python
res = nn_training(X, y)
res
```

> 训练误差已经够低了，不明白为啥还是False 😒

![](https://gitee.com/veal98/images/raw/master/img/20200622164638.png)

最后，我们可以计算准确度，看看我们训练完毕的神经网络效果怎么样。

```python
# 预测值与实际值比较
from sklearn.metrics import classification_report #这个包是评价报告

def accuracy(theta, X, y):
    a1, z2, a2, z3, h = feed_forward(res.x, X)
    y_pred = np.argmax(h, axis=1) + 1
    print(classification_report(y, y_pred))
```

![](https://gitee.com/veal98/images/raw/master/img/20200622161939.png)

#### ⑦ 可视化隐藏层

理解神经网络是如何学习的一个很好的办法是，可视化隐藏层单元所捕获的内容。通俗的说，对于一个隐藏层单元，可视化它所计算的内容的方法是：找到一个输入x，x可以激活这个单元（也就是说有一个激活值 $a^{(l)}_i$ 接近与1）。对于我们所训练的网络，注意到 θ1 中每一行都是一个 401 维的向量，代表每个隐藏层单元的参数。<u>如果我们忽略偏置项，我们就能得到400维的向量，这个向量代表每个样本输入到每个隐层单元的像素的权重。**因此可视化的一个方法是，reshape 这个400维的向量为（20，20）的图像然后输出**。</u>

```python
def plot_hidden(theta):
    t1, t2 = deserialize(theta)
    t1 = t1[:, 1:] # 忽略偏置项
    fig,ax_array = plt.subplots(5, 5, sharex=True, sharey=True, figsize=(6,6))
    for r in range(5):
        for c in range(5):
            ax_array[r, c].matshow(t1[r * 5 + c].reshape(20, 20), cmap='gray_r')
            plt.xticks([])
            plt.yticks([])
    plt.show()
```

![](https://gitee.com/veal98/images/raw/master/img/20200622162306.png)

## 🚀 Ex5：偏差和方差

> 🔊 在本练习中，您将实现正则化的线性回归和多项式回归，并使用它来研究具有不同偏差 -方差属性的模型

### 1. 正则化线性回归

在前半部分的练习中，你将实现正则化线性回归，以预测水库中的水位变化，从而预测大坝流出的水量。在下半部分中，您将通过一些调试学习算法的诊断，并检查偏差 v.s. 方差的影响。

```python
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
```

#### ① 可视化数据

我们将从可视化数据集开始，其中包含水位变化的历史记录 X，以及从大坝流出的水量 y。

这个数据集分为了三个部分：

- 训练集：训练模型
- 交叉验证集：选择正则化参数
- 测试集：评估性能，模型训练中不曾用过的样本

```python
path = 'ex5/ex5data1.mat'
data = loadmat(path)

# Training set
X, y = data['X'], data['y']
# Cross validation set
Xval, yval = data['Xval'], data['yval']
# Test set
Xtest, ytest = data['Xtest'], data['ytest']

# 插入偏置项
X = np.insert(X,0,1,axis = 1)
Xval = np.insert(Xval ,0,1,axis=1)
Xtest = np.insert(Xtest,0,1,axis=1)
```

![](https://gitee.com/veal98/images/raw/master/img/20200623111426.png)

```python
def plotData():
    """瞧一瞧数据长啥样"""
    plt.figure(figsize=(8,5))
    plt.scatter(X[:,1:], y, c='r', marker='x')
    plt.xlabel('Change in water level (x)')
    plt.ylabel('Water flowing out of the dam (y)')
    
plotData()
```

![](https://gitee.com/veal98/images/raw/master/img/20200623111803.png)

#### ② 正则化线性回归的代价函数

<img src="https://gitee.com/veal98/images/raw/master/img/20200623111845.png" style="zoom:80%;" />

```python
def costReg(theta, X, y, l):
    '''do not regularizethe theta0
    theta is a 1-d array with shape (n+1,)
    X is a matrix with shape (m, n+1)
    y is a matrix with shape (m, 1)
    '''
    cost = np.sum((X @ theta - y.flatten()) ** 2)
    regterm = l * (theta[1:] @ theta[1:]) # 不需要正则化 θ_0
    return (cost + regterm) / (2 * len(X))
```

θ 初始值为 [1,1]，λ = 1

```python
theta = np.ones(X.shape[1]) # theta初始值为[1,1]
print(costReg(theta, X, y, 1))  # 303.9931922202643
```

![](https://gitee.com/veal98/images/raw/master/img/20200623112438.png)

#### ③ 正则化线性回归的梯度

<img src="https://gitee.com/veal98/images/raw/master/img/20200623112509.png" style="zoom:80%;" />

```python
def gradientReg(theta, X, y, l):
    """
    theta: 1-d array with shape (2,)
    X: 2-d array with shape (12, 2)
    y: 2-d array with shape (12, 1)
    l: lambda constant
    grad has same shape as theta (2,)
    """
    grad = (X @ theta - y.flatten()) @ X
    regterm = l * theta
    regterm[0] = 0  # #don't regulate bias term
    return (grad + regterm) / len(X)

# Using theta initialized at [1; 1] you should expect to see a 
# gradient of [-15.303016; 598.250744] (with lambda=1)
print(gradientReg(theta, X, y, 1)) # [-15.30301567 598.25074417]
```

![](https://gitee.com/veal98/images/raw/master/img/20200623113044.png)

#### ④ 拟合线性回归

```python
def trainLinearReg(X, y, l):
    theta = np.zeros(X.shape[1])
    res = opt.minimize(fun=costReg, 
                       x0=theta, 
                       args=(X, y ,l), 
                       method='TNC', 
                       jac=gradientReg)
    return res.x
```

```python
fit_theta = trainLinearReg(X, y, 0)
plotData()
plt.plot(X[:,1], X @ fit_theta) # x轴是 输入数据 X，y轴是假设函数 h(θ) = θ^T * X = X * θ
```

![](https://gitee.com/veal98/images/raw/master/img/20200623115402.png)

**这里我们把 `λ  = 0`，因为我们现在实现的线性回归只有两个参数，这么低的维度，正则化并没有用**。

从图中可以看到，拟合最好的这条直线告诉我们这个模型并不适合这个数据。

在下一节中，您将实现一个函数来生成学习曲线，它可以帮助您调试学习算法，即使可视化数据不那么容易。

### 2. 偏差和方差

偏差较大的模型会欠拟合，而方差较大的模型会过拟合。这部分会让你画出学习曲线来判断方差和偏差的问题。

#### ① 绘制学习曲线

![](https://gitee.com/veal98/images/raw/master/img/20200623115531.png)

训练样本 X 从 1 开始逐渐增加，训练出不同的参数向量 θ。接着通过交叉验证样本 Xval 计算验证误差。

- 使用训练集的子集来训练模型，得到不同的 θ。

- 通过 θ 计算训练代价和交叉验证代价，切记此时不要使用正则化， λ = 0。

- 计算交叉验证代价时记得整个交叉验证集来计算，无需分为子集。

画出学习曲线，即交叉验证误差和训练误差随样本数量的变化的变化：

```python
def plot_learning_curve(X, y, Xval, yval, l):
    """画出学习曲线，即交叉验证误差和训练误差随样本数量的变化的变化"""
    xx = range(1, len(X) + 1)  # at least has one example 
    training_cost, cv_cost = [], []
    for i in xx:
        res = trainLinearReg(X[:i], y[:i], l) # 通过训练集数据计算 θ
        training_cost_i = costReg(res, X[:i], y[:i], 0) # 通过 θ 计算训练集代价
        cv_cost_i = costReg(res, Xval, yval, 0) # 通过 θ 计算验证集代价
        training_cost.append(training_cost_i)
        cv_cost.append(cv_cost_i)
        
    plt.figure(figsize=(8,5))
    plt.plot(xx, training_cost, label='training cost')  
    plt.plot(xx, cv_cost, label='cv cost') 
    plt.legend()
    plt.xlabel('Number of training examples')
    plt.ylabel('Error')
    plt.title('Learning curve for linear regression')
    plt.grid(True) # 显示对齐格子
```

![](https://gitee.com/veal98/images/raw/master/img/20200623120540.png)

从图中看出来，随着样本数量的增加，训练误差和交叉验证误差都很高，属于高偏差，欠拟合。

### 3. 多项式回归

我们的线性模型对于数据来说太简单了，导致了欠拟合(高偏差)。在这一部分的练习中，您将通过添加更多的特征来解决这个问题。

使用多项式回归，假设函数形式如下：

![](https://gitee.com/veal98/images/raw/master/img/20200623120841.png)

#### ① 数据预处理

- 使用之前的代价函数和梯度函数
- 扩展特征到 8 阶特征（**这里我们选择增加到 6 次方**，若选 8 次方无法达到作业 pdf 上的效果图，这是因为 `scipy `和 `octave `版本的优化算法不同。）
- 使用 **均值归一化** 来处理 xn
- λ = 0

```python
def genPolyFeatures(X, power):
    """添加多项式特征
    每次在array的最后一列插入第二列的i+2次方（第一列为偏置）
    从二次方开始开始插入（因为本身含有一列一次方）
    """
    Xpoly = X.copy()
    for i in range(2, power + 1):
        Xpoly = np.insert(Xpoly, Xpoly.shape[1], np.power(Xpoly[:,1], i), axis=1)
    return Xpoly

def get_means_std(X):
    """获取训练集的均值和误差，用来标准化所有数据。"""
    means = np.mean(X,axis=0)
    stds = np.std(X,axis=0,ddof=1)  # ddof=1 means 样本标准差
    return means, stds

def featureNormalize(myX, means, stds):
    """标准化"""
    X_norm = myX.copy()
    X_norm[:,1:] = X_norm[:,1:] - means[1:]
    X_norm[:,1:] = X_norm[:,1:] / stds[1:]
    return X_norm
```

> 💡 关于归一化，所有数据集应该都用训练集的均值和样本标准差处理。切记。所以**要将训练集的均值和样本标准差存储起来，对后面的数据进行处理**。
>
> 而且**注意这里是样本标准差而不是总体标准差**，使用`np.std()`时，将**`ddof=1`则是样本标准差，默认`=0`是总体标准差**。而`pandas`默认计算样本标准差。

获取添加多项式特征以及标准化之后的数据：

```python
power = 6  # 扩展到x的6次方

train_means, train_stds = get_means_std(genPolyFeatures(X,power))

# 数据集数据标准化
X_norm = featureNormalize(genPolyFeatures(X,power), train_means, train_stds)
# 验证集数据标准化
Xval_norm = featureNormalize(genPolyFeatures(Xval,power), train_means, train_stds)
# 测试集数据标准化
Xtest_norm = featureNormalize(genPolyFeatures(Xtest,power), train_means, train_stds)
```

```python
def plot_fit(means, stds, l):
    """画出拟合曲线"""
    theta = trainLinearReg(X_norm,y, l)
    x = np.linspace(-75,55,50)
    xmat = x.reshape(-1, 1)
    xmat = np.insert(xmat,0,1,axis=1) # 添加偏置项
    Xmat = genPolyFeatures(xmat, power) # 添加多项式特征
    Xmat_norm = featureNormalize(Xmat, means, stds) # 均值归一化
    
    plotData()
    plt.plot(x, Xmat_norm@theta,'b--') # 绘制拟合曲线
```

> 💡 `np.linspace`函数：`np.linspace` 主要用来创建等差数列。
>
> ```python
> np.linspace(2.0, 3.0, num=5)
> # array([ 2.  ,  2.25,  2.5 ,  2.75,  3.  ])
> ```

```python
plot_fit(train_means, train_stds, 0)
```

![](https://gitee.com/veal98/images/raw/master/img/20200623123415.png)

学习曲线：

```python
plot_learning_curve(X_norm, y, Xval_norm, yval, 0)
```

![](https://gitee.com/veal98/images/raw/master/img/20200623123524.png)

可以看到 λ  = 0 时，训练误差太小了，明显过拟合了。

#### ② 调整正则化系数 λ

👉 我们继续调整 `λ =  1` 时：

拟合多项式线性回归：

```python
plot_fit(train_means, train_stds, 1)
```

![](https://gitee.com/veal98/images/raw/master/img/20200623123932.png)

学习曲线：

```python
plot_learning_curve(X_norm, y, Xval_norm, yval, 1)
```

![](https://gitee.com/veal98/images/raw/master/img/20200623123942.png)

训练代价不再是0了，也就是说我们**过拟合**程度减轻了

👉 如果 `λ = 100` ：

```python
plot_fit(train_means, train_stds, 1)
```

![](https://gitee.com/veal98/images/raw/master/img/20200623124135.png)

```python
plot_learning_curve(X_norm, y, Xval_norm, yval, 1)
```

![](https://gitee.com/veal98/images/raw/master/img/20200623124143.png)

太多正则化，导致训练集和验证集误差都很高，即欠拟合

#### ③ 通过交叉验证集选择最佳的 λ

通过之前的实验，我们可以发现 λ 可以极大程度地影响正则化多项式回归。 所以这部分我们会会使用验证集去评价 λ 的表现好坏，然后选择表现最好的 λ 后，用测试集测试模型在没有出现过的数据上会表现多好。 尝试 λ 值[0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]

```python
lambdas = [0., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1., 3., 10.]
errors_train, errors_val = [], []
for l in lambdas:
    theta = trainLinearReg(X_norm, y, l)
    errors_train.append(costReg(theta,X_norm,y,0))  # 训练集误差 记得把lambda = 0
    errors_val.append(costReg(theta,Xval_norm,yval,0)) # 验证集误差
    
plt.figure(figsize=(8,5))
plt.plot(lambdas,errors_train,label='Train')
plt.plot(lambdas,errors_val,label='Cross Validation')
plt.legend()
plt.xlabel('lambda')
plt.ylabel('Error')
plt.grid(True)
```

![](https://gitee.com/veal98/images/raw/master/img/20200623124918.png)

选择使得交叉验证集误差/代价 最小的 λ

```python
# 可以看到交叉验证代价最小的是 lambda = 3
lambdas[np.argmin(errors_val)]  # 3.0
```

![](https://gitee.com/veal98/images/raw/master/img/20200623125010.png)

#### ④ 计算测试集上的误差

实际上，为了获得一个更好的模型，我们需要把最终的模型用在一个从来没有在计算中出现过的测试集上，也就是说，需要既没有被用作选择 θ，也没有被用作选择 λ 的数据

```python
theta = trainLinearReg(X_norm, y, 3) # 通过训练集训练出 θ
print('test cost(l={}) = {}'.format(3, costReg(theta, Xtest_norm, ytest, 0))) # 通过 θ 计算测试集误差/代价
```

![](https://gitee.com/veal98/images/raw/master/img/20200623125550.png)

> 🔈 因为我们在上面调整了`power = 6`来匹配作业里面的图，所以得不到 3.8599。但是调整 `power=8` 时（同作业里一样）, 就可以得到上述数据 3.8599。

## 🚀 Ex6：支持向量机 SVM

### 1. 支持向量机

```python
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from scipy.io import loadmat
from sklearn import svm
```

#### ① 数据集 1

```python
mat = loadmat('ex6/ex6data1.mat')
print(mat.keys())
# dict_keys(['__header__', '__version__', '__globals__', 'X', 'y'])
X = mat['X']
y = mat['y']
```

大多数SVM的库会自动帮你添加额外的特征 x0 以及 θ0 ，所以无需手动添加。

```python
def plotData(X, y):
    plt.figure(figsize=(8,5))
    plt.scatter(X[:,0], X[:,1], c=y.flatten(), cmap='rainbow')
    plt.xlabel('X1')
    plt.ylabel('X2')
plotData(X, y)
```

![](https://gitee.com/veal98/images/raw/master/img/20200626105512.png)

训练线性支持向量机找出边界：

```python
def plotBoundary(clf, X):
    '''plot decision bondary'''
    x_min, x_max = X[:,0].min()*1.2, X[:,0].max()*1.1
    y_min, y_max = X[:,1].min()*1.1,X[:,1].max()*1.1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),
                         np.linspace(y_min, y_max, 500))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contour(xx, yy, Z)

```

```python
models = [svm.SVC(C, kernel='linear') for C in [1, 100]]
clfs = [model.fit(X, y.ravel()) for model in models]
```

```python
title = ['SVM Decision Boundary with C = {} (Example Dataset 1'.format(C) for C in [1, 100]]
for model,title in zip(clfs,title):
    plt.figure(figsize=(8,5))
    plotData(X, y)
    plotBoundary(model, X)
    plt.title(title)
```

![](https://gitee.com/veal98/images/raw/master/img/20200626110513.png)

可以从上图看到，当C比较小时模型对误分类的惩罚增大，比较严格，误分类少，间隔比较狭窄。

当 C 比较大时模型对误分类的惩罚增大，比较宽松，允许一定的误分类存在，间隔较大。

#### ② 高斯内核的 SVM

这部分，使用SVM做非线性分类。我们将使用高斯核函数。

![](https://gitee.com/veal98/images/raw/master/img/20200626110929.png)

这里我们用 sklearn 自带的 svm 中的核函数即可。

##### Ⅰ 高斯内核

```python
def gaussKernel(x1, x2, sigma):
    return np.exp(- ((x1 - x2) ** 2).sum() / (2 * sigma ** 2))

gaussKernel(np.array([1, 2, 1]),np.array([0, 4, -1]), 2.)  # 0.32465246735834974
```

##### Ⅱ 数据集 2

```python
mat = loadmat('ex6/ex6data2.mat')
X2 = mat['X']
y2 = mat['y']

plotData(X2, y2)
```

![](https://gitee.com/veal98/images/raw/master/img/20200626111240.png)

使用高斯内核的 SVM 找出非线性边界：

```python
sigma = 0.1
gamma = np.power(sigma,-2.)/2
clf = svm.SVC(C=1, kernel='rbf', gamma=gamma)
modle = clf.fit(X2, y2.flatten())
plotData(X2, y2)
plotBoundary(modle, X2)
```

![](https://gitee.com/veal98/images/raw/master/img/20200626111555.png)

##### Ⅲ 数据集 3

对于第三个数据集，我们给出了训练和验证集，并且基于验证集性能为SVM模型找到最优超参数。 我们现在需要寻找最优C和σ，候选数值为[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]

```python
mat3 = loadmat('ex6/ex6data3.mat')
X3, y3 = mat3['X'], mat3['y']
Xval, yval = mat3['Xval'], mat3['yval']

plotData(X3, y3)
```

![](https://gitee.com/veal98/images/raw/master/img/20200626111729.png)

```python
Cvalues = (0.01, 0.03, 0.1, 0.3, 1., 3., 10., 30.)
sigmavalues = Cvalues
best_pair, best_score = (0, 0), 0

for C in Cvalues:
    for sigma in sigmavalues:
        gamma = np.power(sigma,-2.)/2
        model = svm.SVC(C=C,kernel='rbf',gamma=gamma)
        model.fit(X3, y3.flatten())
        this_score = model.score(Xval, yval)
        if this_score > best_score:
            best_score = this_score
            best_pair = (C, sigma)
print('best_pair={}, best_score={}'.format(best_pair, best_score))
# best_pair=(1.0, 0.1), best_score=0.965
```

👉 可以看出 C 和 σ 的最佳值分别为 1.0 和 0.1

画出分界线：

```python
model = svm.SVC(C=1., kernel='rbf', gamma = np.power(.1, -2.)/2)
model.fit(X3, y3.flatten())

plotData(X3, y3)
plotBoundary(model, X3)
```

![](https://gitee.com/veal98/images/raw/master/img/20200626112113.png)

### 2. 垃圾邮件分类

在这一部分中，我们的目标是用SVM建立一个垃圾邮件分类器。你需要将每个email 变成一个n维的特征向量，这个分类器将判断给定一个邮件 x 是垃圾邮件(y=1)或不是垃圾邮件(y=0)。

#### ① 处理邮件

##### Ⅰ 预处理

首先我们从数据集中选取一个邮件来看看具体包含什么内容：

```python
with open('ex6/emailSample1.txt', 'r') as f:
    email = f.read()
    print(email)
```

![](https://gitee.com/veal98/images/raw/master/img/20200626112620.png)

可以看到，邮件内容包含 a URL, an email address(at the end), numbers, and dollar amounts. 很多邮件都会包含这些元素，但是每封邮件的具体内容可能会不一样。因此，**处理邮件经常采用的方法是标准化这些数据，把所有URL当作一样，所有数字看作一样。**

例如，我们**用唯一的一个字符串`httpaddr`来替换所有的URL，来表示邮件包含URL**，而不要求具体的URL内容。这通常会提高垃圾邮件分类器的性能，因为垃圾邮件发送者通常会随机化URL，因此在新的垃圾邮件中再次看到任何特定URL的几率非常小。

我们可以做如下处理：

- Lower-casing: 把整封邮件转化为小写。

- Stripping HTML: 移除所有HTML标签，只保留内容。

- Normalizing URLs: 将所有的URL替换为字符串 “httpaddr”.

- Normalizing Email Addresses: 所有的地址替换为 “emailaddr”

- Normalizing Dollars: 所有dollar符号($)替换为“dollar”.

- Normalizing Numbers: 所有数字替换为“number”

- Word Stemming(词干提取): 将所有单词还原为词源。例如，“discount”, “discounts”, “discounted” and “discounting”都替换为“discount”。

- Removal of non-words: 移除所有非文字类型，所有的空格(tabs, newlines, spaces)调整为一个空格.

```python
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
from sklearn import svm
import re #regular expression for e-mail processing

# 英文分词算法
import nltk, nltk.stem.porter
```

利用正则表达式完成 除了 词干提取 和 移除所有非文字类型 的所有邮件处理：

```python
def processEmail(email):
    """做除了 词干提取 和 移除所有非文字类型 的所有处理"""
    email = email.lower()
    email = re.sub('<[^<>]>', ' ', email)  # 匹配<开头，然后所有不是< ,> 的内容，知道>结尾，相当于匹配<...>
    email = re.sub('(http|https)://[^\s]*', 'httpaddr', email )  # 匹配//后面不是空白字符的内容，遇到空白字符则停止
    email = re.sub('[^\s]+@[^\s]+', 'emailaddr', email)
    email = re.sub('[\$]+', 'dollar', email)
    email = re.sub('[\d]+', 'number', email) 
    return email
```

接下来就是提取词干，以及去除非字符内容：

```python
def email2TokenList(email):
    """预处理数据，返回一个干净的单词列表"""
    
    # I'll use the NLTK stemmer because it more accurately duplicates the
    # performance of the OCTAVE implementation in the assignment
    stemmer = nltk.stem.porter.PorterStemmer()
    
    email = processEmail(email)

    # 将邮件分割为单个单词，re.split() 可以设置多种分隔符
    tokens = re.split('[ \@\$\/\#\.\-\:\&\*\+\=\[\]\?\!\(\)\{\}\,\'\"\>\_\<\;\%]', email)
    
    # 遍历每个分割出来的内容
    tokenlist = []
    for token in tokens:
        # 删除任何非字母数字的字符
        token = re.sub('[^a-zA-Z0-9]', '', token);
        # Use the Porter stemmer to 提取词根
        stemmed = stemmer.stem(token)
        # 去除空字符串‘’，里面不含任何字符
        if not len(token): continue
        tokenlist.append(stemmed)
            
    return tokenlist  
```

##### Ⅱ 词汇表

在对邮件进行预处理之后，我们有一个处理后的单词列表。下一步是选择我们想在分类器中使用哪些词，我们需要去除哪些词。

我们有一个词汇表 `vocab.txt`，里面存储了在实际中经常使用的单词，共1899个。

我们要算出处理后的email中含有多少vocab.txt中的单词，并返回在vocab.txt中的index，这就我们想要的训练单词的索引。

```python
def email2VocabIndices(email, vocab):
    """提取存在单词的索引"""
    token = email2TokenList(email)
    index = [i for i in range(len(vocab)) if vocab[i] in token ]
    return index
```

#### ② 提取特征

```python
def email2FeatureVector(email):
    """
    将email转化为词向量，n是vocab的长度。存在单词的相应位置的值置为1，其余为0
    """
    df = pd.read_table('ex6/vocab.txt',names=['words'])
    vocab = df.as_matrix()  # return array
    vector = np.zeros(len(vocab))  # init vector
    vocab_indices = email2VocabIndices(email, vocab)  # 返回含有单词的索引
    # 将有单词的索引置为1
    for i in vocab_indices:
        vector[i] = 1
    return vector

vector = email2FeatureVector(email)
print('length of vector = {}\nnum of non-zero = {}'.format(len(vector), int(vector.sum())))
```

![](https://gitee.com/veal98/images/raw/master/img/20200626113801.png)

每个文档已经转换为一个向量，其中1,899个维对应于词汇表中的1,899个单词。

#### ③ 训练 SVM 用于邮件分类

读取已经训提取好的特征向量以及相应的标签。分训练集和测试集。

```python
# Training set
mat1 = loadmat('ex6/spamTrain.mat')
X, y = mat1['X'], mat1['y']

# Test set
mat2 = loadmat('ex6/spamTest.mat')
Xtest, ytest = mat2['Xtest'], mat2['ytest']

clf = svm.SVC(C=0.1, kernel='linear')
clf.fit(X, y)
```

![](https://gitee.com/veal98/images/raw/master/img/20200626114207.png)

看看训练的准确度：

```python
predTrain = clf.score(X, y)
predTest = clf.score(Xtest, ytest)
predTrain, predTest # (0.99825, 0.989)
```

![](https://gitee.com/veal98/images/raw/master/img/20200626114638.png)