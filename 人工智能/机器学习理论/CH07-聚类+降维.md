# 第七章 聚类和降维

## 7.1 聚类 (Clustering)

### 7.1.1 无监督学习：简介 Unsupervised Learning_ Introduction

<u>在本节中，我们将开始学习**聚类算法**，这是我们学习的第一个**非监督学习算法**</u>。我们将要让计算机学习**无标签**数据，而不是此前的标签数据。

![](https://gitee.com/wugenqiang/images/raw/master/02/20200612091451.png)



我们将先介绍聚类算法。此后，我们将陆续介绍其他算法。❓ 那么聚类算法一般用来做什么呢？

![](https://gitee.com/wugenqiang/images/raw/master/02/20200612091542.png)



### 7.1.2 K-均值算法 K-Means Algorithm

🌌 **K - 均值** 是最普及的聚类算法，**算法接受一个未标记的数据集，然后将数据聚类成不同的组**。

**K - 均值** 是一个迭代算法，假设我们想要将数据聚类成 n 个组，其方法为：

- 步骤 1：首先选择 K 个随机的点，称为**聚类中心**（**cluster centroids**）；

  <img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612093349.png" style="zoom:50%;" />

- 步骤 2 - **簇分类**：对于数据集中的每一个数据，按照距离 K 个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。

  <img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612093509.png" style="zoom:50%;" />

- 步骤 3 - **移动聚类中心**：计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。

  <img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612093536.png" style="zoom:50%;" />

- 步骤 4：重复步骤 2 - 4 **直至聚类中心点不再变化**。

  <img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612093620.png" style="zoom:50%;" />

  <img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612093634.png" style="zoom:50%;" />

用$μ^1$,$μ^2$,...,$μ^k$ 来表示聚类中心，用$c^{(1)}$,$c^{(2)}$,...,$c^{(m)}$来存储与第$i$个实例数据最近的聚类中心的索引，**K - 均值** 算法的伪代码如下：

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612094133.png" style="zoom:50%;" />

**K-均值**算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用**K-均值**算法将数据分为三类，用于帮助确定将要生产的T-恤衫的三种尺寸。

### 7.1.3 优化目标 Optimization Objective 

K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的代价函数（又称**畸变函数** **Distortion function**）为：

![](https://gitee.com/wugenqiang/images/raw/master/02/20200612094746.png)

其中${{\mu }_{{{c}^{(i)}}}}$代表与${{x}^{(i)}}$最近的聚类中心点。 我们的的优化目标便是找出使得代价函数最小的 $c^{(1)}$,$c^{(2)}$,...,$c^{(m)}$和$μ^1$,$μ^2$,...,$μ^k$：

 ![](https://gitee.com/wugenqiang/images/raw/master/02/20200612094621.png)

回顾刚才给出的: **K - 均值** 迭代算法，我们知道，🚩 **第一个循环是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小${{\mu }_{i}}$引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误**。

### 7.1.4 随机初始化 Random Initialization

在运行K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做：

- 应该选择$K<m$，即聚类中心点的个数要小于所有训练集实例的数量

- ⭐ **随机选择$K$个训练实例，然后令$K$个聚类中心分别与这$K$个训练实例相等**

  <img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612095323.png" style="zoom:50%;" />

显然，🚩 **随机初始化的不同，K-均值算法得到的结果也会不同，它有可能会停留在一个局部最小值处。**如下图所示：

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612095634.png" style="zoom:50%;" />



为了解决这个问题，我们通常需要<u>多次运行 **K-均值** 算法，每一次都重新进行随机初始化，最后再比较多次运行 **K-均值** 的结果，选择代价函数最小的结果。</u>这种方法在 $K$ 较小的时候（2--10）还是可行的，但是如果 $K$ 较大，这么做也可能不会有明显地改善。 

### 7.1.5 选择聚类数量(K)

① <u>选择聚类数量 / 选择参数 K 的值的方法：**“肘部法则”**。</u>

> 显然肘部法则并不能在每种场合都奏效，😔 实际上**最常见的方法仍然是手动选择 K** （by hand）

关于“肘部法则”，我们所需要做的是**不断改变$K$值**，也就是聚类类别数目的总数。首先我们用一个聚类来运行 **K-均值** 聚类方法，这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数 $J$。$K$ 代表聚类数字。

![](https://gitee.com/wugenqiang/images/raw/master/02/20200612095804.png)

我们可能会得到一条类似于这样的曲线。像一个人的肘部。它的畸变值会迅速下降，从1到2，从2到3之后，你会在3的时候达到一个**肘点**。在此之后，畸变值就下降的非常慢，那么我们就选$K=3$。

② **⭐ 手动选择 K** 的方法：**有些时候我们运行 K-means 算法是为了达到某些后续目的，那么我们就需要评估我们选择的 K 或者说我们进行的分类能否较好的达成后续目的。**

例如，我们的 T-恤制造例子中，我们要将用户按照身材聚类，我们可以分成3个尺寸:$S,M,L$，也可以分成5个尺寸$XS,S,M,L,XL$，<u>到底分成 3 类还是 5 类是基于“聚类后我们制造的T-恤是否能较好地适合我们的客户”这个问题的基础上来做决定的。通俗来说，就是如果把尺寸分成 3/5 类，我可以卖出多少 T 恤？</u>

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612101646.png" style="zoom:50%;" />

## ✍ Quiz

### ① 第 1 题

对于以下哪些任务，K-means 聚类可能是一种合适的算法？选出所有正确项

- ✅ 给定一个关于用户信息的数据库，自动将用户分组到不同的市场细分中。

- ✅ 根据超市中大量产品的销售数据，找出哪些产品可以组成组合（比如经常一起购买），因此应该放在同一个货架上。

- 根据历史天气记录，预测明天的降雨量

- 给定超市中大量产品的销售数据，估计这些产品的未来销售额。

- ✅ 给出一组来自许多不同新闻网站的新闻文章，找出所涉及的主要主题。

- 基于许多电子邮件，确定它们是垃圾邮件还是非垃圾邮件。

- ✅ 从网站上的用户使用模式，找出哪些不同的用户群体存在。

- 根据历史天气记录，预测明天的天气是晴还是雨。

### ② 第 2 题

假设我们有三个簇中心 $\mu_1 = \begin{bmatrix}1 \\ 2 \end{bmatrix},\mu_2 = \begin{bmatrix}-3 \\ 0 \end{bmatrix},\mu_3 = \begin{bmatrix}4 \\ 2 \end{bmatrix}$。此外，我们还有一个训练示例 $x^{(i)} = \begin{bmatrix}-2 \\ 1 \end{bmatrix}$。在一个集群分配步骤之后，$c^{(i)}$ 将是什么？

- ✅ $c^{(i)}$ = 2 
- $c^{(i)}$ 未被分配 
- $c^{(i)}$=1 
- $c^{(i)}$=3

> 💡 <img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612103846.png" style="zoom: 80%;" />

### ③ 第 3 题

K-means 是一种迭代算法，在其内部循环中重复执行以下两个步骤。哪两个？

- ✅ 移动簇中心，更新簇中心 $μ_k$。

- ✅ 分配簇，其中参数 $c^{(i)}$ 被更新。

- 移动簇中心 $μ_k$，将其设置为等于最近的训练示例 $c^{(i)}$

- 簇中心分配步骤，其中每个簇质心 $μ_i$ 被分配（通过设置c(i)）到最近的训练示例 $x^{(i)}$。

### ④ 第 4 题

假设您有一个未标记的数据集 $\{x^{(1)}, \ldots,  x^{(m)}\}$。你用 50 个不同的随机数运行 K-means 初始化，并获得了 50 个不同的聚类。选择这 50 个组合中的哪一个的方法是什么 ？

- 唯一的方法是我们需要数据标签 $y^{(i)}$。

- ✅ 对于每一个分类，计算 $\frac{1}{m} \sum_{i=1}^m{ ||x^{(i)} - \mu_{c^{(i)}}||^2}$，并选择这个值最小的一个。

- 答案模棱两可，没有好的选择方法。

- 总是选择找到的最后一个（第50个）聚类，因为它更有可能收敛到一个好的解决方案。

### ⑤ 第 5 题

以下哪项陈述是正确的？选出所有正确项

- ✅ 如果我们担心K-means陷入局部最优解，一种改善（减少）这个问题的方法是尝试使用多个随机初始化。

- 初始化 K-均值 的标准方法是将 $\mu_1=...=\mu_k$ 设置为等于零的向量。

- 由于K-Means是一种无监督的学习算法，它不能对数据进行过度拟合，因此最好在计算上尽可能多的聚类。

- ✅ 对于某些数据集，K（集群数量）的“正确”值可能是不明确的，甚至对于仔细查看数据的人类专家来说也很难做出决定。

- 无论簇中心的初始化如何，K-均值都会给出相同的结果。

- ✅ 初始化K-means的一个好方法是从训练集中选择K个（不同的）示例，并设置与这些选定示例相等的簇质心。

- ✅ 在K-均值的每次迭代中，代价函数 $J(c^{(1)}, \ldots, c^{(m)}, \mu_1, \ldots,\mu_k)$ （失真函数）要么保持不变，要么减小，特别是不应增加。

- 一旦一个例子被分配到一个特定的簇中心，它将永远不会被重新分配到另一个不同的簇中心。



## 7.2 降维(Dimensionality Reduction)

### 7.2.1 目标一：数据压缩 Data Compression

<u>本节中，我们开始学习第二种类型的**无监督学习**问题，称为**降维**</u>。有几个不同的的原因使你可能想要做降维。一是数据压缩，<u>数据压缩不仅允许我们压缩数据，使用较少的计算机内存或磁盘空间，它还能加速我们的学习算法。</u>

❓ **降维是什么？**

假设我们未知两个的特征：<u>$x_1$:长度：用厘米表示；$x_2$：是用英寸表示同一物体的长度。</u>

![](https://gitee.com/wugenqiang/images/raw/master/02/20200612105334.png)

所以，这给了我们高度**冗余**表示，$x_1$和$x_2$不是完全分开的两个特征，我们想要做的是减少数据到一维，只有一个数测量这个长度。

👉 将数据从二维降至一维： 假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，<u>两种仪器对同一个东西测量的结果不完全相等（由于误差、精度等），而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。</u>

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612105803.png" style="zoom:50%;" />

将数据从三维降至二维： 这个例子中我们要将一个三维的特征向量降至一个二维的特征向量。过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的数据都在同一个平面上，降至二维的特征向量。

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612110013.png" style="zoom:50%;" />

这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将1000维的特征降至100维。

### 7.2.2 目标二：数据可视化 Visualization

在许多机器学习问题中，如果我们能将**数据可视化**，我们便能寻找到一个更好的解决方案，降维可以帮助我们。

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612110232.png" style="zoom:50%;" />

假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如**GDP**，人均**GDP**，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。

> 💡 比如可以 令 z1 表示 国家面积 / GDP，令 z2 表示人口总数 / GDP，用这两个指标代表整个国家

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612110337.png" style="zoom: 43%;" />



<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612110549.png" style="zoom: 50%;" />

🔈 这样做的问题在于，**降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了**。

### 7.2.3 主成分分析算法的目标 Principal Component Analysis

⭐ **主成分分析(PCA)是最常见的降维算法**。

在**PCA**中，我们要做的是找到一个方向向量（**Vector direction**），当我们把所有的数据都投射到该向量上时，🚩 **我们希望投射平均均方误差能尽可能地小**。方向向量是一个经过原点的向量，而**投射误差 Projection Error 是从特征向量向该方向向量作垂线的长度。**

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612111540.png" style="zoom:50%;" />

下面给出主成分分析问题的描述：

问题是要将$n$维数据降至$k$维，目标是找到向量$u^{(1)}$,$u^{(2)}$,...,$u^{(k)}$使得总的投射误差最小。

🚩 **主成分分析与线性回归的比较**：

- 主成分分析与线性回归是两种不同的算法。

- 主成分分析最小化的是投射误差（**Projected Error**），而线性回归尝试的是最小化预测误差。

- 线性回归的目的是预测结果，而主成分分析不作任何预测。

![](https://gitee.com/wugenqiang/images/raw/master/02/20200612111855.png)

⭐ 上图中，左边的是**线性回归的误差（垂直于横轴投影）**，右边则是主**要成分分析的误差（垂直于红线投影）**。

**PCA**将$n$个特征降维到$k$个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。但**PCA 要保证降维后，还要保证数据的特性损失最小**。

**PCA**技术的一大好处是对数据进行降维的处理。<u>我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</u>

**PCA**技术的一个很大的优点是，它是完全无参数限制的。在**PCA**的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。

但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。

### 7.2.4 主成分分析算法的具体实现

**PCA** 减少 $n$ 维到 $k$ 维：

- 第一步是均值归一化。我们需要计算出所有特征的均值，然后令 $x_j= x_j-μ_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $σ^2$。

- 第二步是计算**协方差矩阵**（**covariance matrix**）$Σ$： $\sum=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$

- 第三步是计算协方差矩阵$Σ$的**特征向量**（**eigenvectors**）:

  在 **Octave** 里我们可以利用**奇异值分解**（**singular value decomposition**）来求解，`[U, S, V]= svd(sigma)`。

  ![](https://gitee.com/wugenqiang/images/raw/master/02/20200612112308.png)

  对于一个 $n×n$维度的矩阵，上式中的$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。

  **如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示**，然后通过如下计算获得要求的新特征向量 $z^{(i)}$: ⭐   $$z^{(i)}=U^{T}_{reduce}*x^{(i)}$$。其中$x$是$n×1$维的，因此结果为$k×1$维度。

  > 注：我们不对方差特征进行处理。

### 7.2.5 选择主成分的数量 (k)

PCA 是减少投射的平均均方误差：

训练集的方差为：$\dfrac {1}{m}\sum^{m}_{i=1}\left| x^{\left( i\right) }\right| ^{2}$

**我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的 $k$ 值。（从 n 个特征降到 k 个特征）**

如果**我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。**

我们可以先令$k=1$，然后进行主成分分析，获得$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k=2$，如此类推，直到找到可以使得比例小于1%的最小$k$ 值（原因是各个特征之间通常情况存在某种相关性）。

还有一些更好的方式来选择 $k$，当我们在**Octave**中调用“**svd**”函数的时候，我们获得三个参数：`[U, S, V] = svd(sigma)`。

![](https://gitee.com/wugenqiang/images/raw/master/02/20200612151436.png)

其中的 $S$ 是一个 $n×n$ 的矩阵，只有对角线上有值，而其它单元都是 0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例： 

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612151639.png"  />

也就是：![](https://gitee.com/wugenqiang/images/raw/master/02/20200612151719.png)

> 💡 在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：$$x^{\left( i\right) }_{approx}=U_{reduce}z^{(i)}$$

### 7.2.6 重建原始数据 Reconstruction

**PCA**作为压缩算法，你可能需要把1000维的数据压缩100维特征，或具有三维数据压缩到一二维表示。如果这是一个压缩算法，应该能回到这个压缩表示，回到你原有的高维数据的一种近似。

❓ 所以，给定的$z^{(i)}$，可能100维，怎么回到你原来的表示$x^{(i)}$，可能是1000维的数组？

![](https://gitee.com/wugenqiang/images/raw/master/02/20200612152146.png)

比如给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？$x$为2维，$z$为 1 维，$z=U^{T}_{reduce}x$，⭐ 相反的方程为：$x_{appox}=U_{reduce}\cdot z$,$x_{appox}\approx x$。如图：

![](https://gitee.com/wugenqiang/images/raw/master/02/20200612152241.png)

所以，这就是你从低维表示 $z$ 回到未压缩的表示。我们也把这个过程称为**重建原始数据**。

### 7.2.7 PCA 算法的应用建议

假使我们正在针对一张 100×100 像素的图片进行某个计算机视觉的机器学习，即总共有10000 个特征。

- 第一步是运用主要成分分析将数据压缩至1000个特征

- 然后对训练集运行学习算法

- 在预测时，采用之前学习而来的$U_{reduce}$将输入的特征$x$转换成特征向量$z$，然后再进行预测

注：如果我们有交叉验证集合测试集，也采用对训练集学习而来的$U_{reduce}$。

🚨 **错误的**主要成分分析情况：

- <u>一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。</u>这样做非常不好，不如尝试正则化处理。原因在于**主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征**。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。

- <u>另一个常见的错误是，默认地将主成分分析作为学习过程中的一部分</u>，这虽然很多时候有效果，**最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。**

## ✍ Quiz

### ① 第 1 题

考虑以下二维数据集：

<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612153109.png" style="zoom:50%;" />

下列哪个图片对应的PCA可能返回的 $u^{(1)}$（第一特征向量/第一主成分）的值？选出所有正确项

- ✅ A：<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612153224.png" style="zoom:50%;" />

- ✅ B：<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612153235.png" style="zoom:50%;" />

- C：<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612153245.png" style="zoom:50%;" />

- D：<img src="https://gitee.com/wugenqiang/images/raw/master/02/20200612153256.png" style="zoom:50%;" />

### ② 第 2 题

以下哪一项是选择主成分k数量的合理方法？（n是输入数据的维度mm是输入示例的数量）

- ✅ 选择至少保留99%的方差的k的最小值

- 选择k，使逼近误差 $\frac{1}{m} \sum_{i=1}^m ||x^{(i)} - x^{(i)}_{\rm approx}||^2$。

- 选择至少保留1%的方差的k的最小值

- 选择k为99%的n（即k=0.99∗n四舍五入至最接近的整数）。

### ③ 第 3 题

假设有人告诉你，他们运行主成分分析的方式是“95%的方差被保留”，什么是与此等价的说法？

- $\frac{\frac{1}{m} \sum_{i=1}^m ||x^{(i)}||^2}{\frac{1}{m} \sum_{i=1}^m ||x^{(i)}- x^{(i)}_{\rm approx}||^2} \geq 0.05$

- $\frac{\frac{1}{m} \sum_{i=1}^m ||x^{(i)}||^2}{\frac{1}{m} \sum_{i=1}^m ||x^{(i)}- x^{(i)}_{\rm approx}||^2} \leq 0.05$

- ✅ $\frac{ \frac{1}{m} \sum_{i=1}^m ||x^{(i)}- x^{(i)}_{\rm approx}||^2}{\frac{1}{m} \sum_{i=1}^m ||x^{(i)}||^2} \leq 0.05$

### ④ 第 4 题

以下哪项陈述是正确的？选择所有正确项

- 仅给出$z^{(i)}$和$U_{reduce}$，就没有办法重建$x^{(i)}$的任何合理的近似。

- ✅ 即使所有的输入特征都在非常相似的尺度上，在运行PCA之前，我们仍然应该执行均值归一化（这样每个特征的均值为零）。

- PCA易受局部最优解的影响；尝试多次随机初始化可能会有所帮助。

- ✅ 给定输入数据$x∈R^n$，仅用满足k≤n的k值运行PCA是有意义的（特别是，用k=n运行PCA是可能的，但没有帮助，k>n没有意义）

### ⑤ 第 5 题

以下哪项是PCA的推荐应用？选择所有正确项

- 作为线性回归的替代：对于大多数模型应用，PCA和线性回归给出了基本相似的结果。

- ✅ 数据压缩：减少数据的维数，从而减少占用的内存/磁盘空间。

- 数据可视化：获取二维数据，并在二维中找到不同的绘制方法（使用k=2）。

- ✅ 数据压缩：减少输入数据x(i)的维数，该维数将用于监督学习算法（即，使用PCA以使监督学习算法运行更快）。

## 📚 References

- 🤖 [吴恩达机器学习经典名课【中英字幕】](https://www.bilibili.com/video/BV164411S78V?p=2)

- 💠 [黄海广 - 斯坦福大学2014机器学习教程中文笔记](http://www.ai-start.com/ml2014/)

- 🍧 [90题细品吴恩达《机器学习》，感受被刷题支配的恐惧](https://www.kesci.com/home/project/5e0f01282823a10036b280a7)

- 🥩 [吴恩达机器学习 课后实验 python实现](https://www.kesci.com/home/project/5da16a37037db3002d441810)

- 🍦 [吴恩达机器学习与深度学习作业目录](https://blog.csdn.net/Cowry5/article/details/83302646)